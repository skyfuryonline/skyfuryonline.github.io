# 博客自动化与智能化最终方案 (v3)

**核心目标：**
打造一个全自动、智能化、纯静态的信息聚合与发布系统。该系统能定时抓取信息，利用 LLM 生成总结，并将结果以优雅、快速、无需后端服务的方式展示给用户，同时保证数据的持久化和一致性。

---

### **第一部分：数据持久化**

**问题：** GitHub Actions 的运行环境是临时的，每次运行都是一个全新的环境，导致上一次爬取的数据（用于查重）和缓存会丢失。

**解决方案：将数据提交回 Git 仓库**

此方案能提供 100% 的数据可靠性，完美满足查重和清理逻辑对数据一致性的强需求。

**实现细节：**
1.  **时机**: 在 `.github/workflows/deploy.yml` 工作流中，`Run crawler` 步骤之后，`Build the site` 步骤之前。
2.  **操作**: 新增一个名为 `Commit and Push Data` 的步骤，该步骤将执行以下脚本：
    *   配置 Git 用户名和邮箱（例如 `GitHub Actions Bot`）。
    *   执行 `git add _data/ cache/`，将新生成的数据和缓存添加到暂存区。
    *   使用 `git diff --staged --quiet` 检查是否有文件变动。
    *   **仅当有变动时**，才执行 `git commit -m "chore: Update daily data for $(date +'%Y-%m-%d')"` 和 `git push`。
3.  **防止无限循环**: 自动 `push` 会再次触发工作流，但因为该次触发的 `event_name` 是 `push`，爬虫步骤的 `if` 条件不满足，爬虫会被跳过，从而避免死循环。

---

### **第二部分：LLM 总结与动态页面**

**问题：** 如何在纯静态的 GitHub Pages 环境下，实现“点击文章卡片后，先看 LLM 总结，再决定是否跳转”的动态交互，同时保证 API 密钥的安全和良好的用户体验？

**解决方案：预生成总结 + 前端模态框 (Modal) 展示**

此方案将 LLM 的计算前置到构建阶段，完美契合静态网站的工作模式，无需引入额外的后端服务。

**实现细节：**
1.  **修改爬虫 (`crawlers/main.py`)**: 
    *   在 `main.py` 获取到文章的原始内容后，**立即**调用 LLM API 进行总结。
    *   **API 密钥管理**: 将 LLM API 密钥作为 **GitHub Secret** (例如 `LLM_API_KEY`) 添加到仓库设置中。
    *   在 `deploy.yml` 中，通过 `env` 关键字将该 Secret 安全地作为环境变量传递给 `main.py` 脚本。
    *   将 LLM 返回的总结，连同标题、链接等信息，一起写入 `_data/daily_YYYY-MM-DD.json`。最终的 JSON 结构将包含 `summary` 字段。

2.  **修改前端 (`daily.html`)**: 
    *   **HTML 结构**: 在页面底部，预先放置一个隐藏的 Bootstrap 模态框 (Modal) 结构。这个模态框将包含标题、内容（用于放总结）和操作按钮（如“阅读原文”和“关闭”）。
    *   **JavaScript 交互**: 
        *   为所有的文章卡片 (`<a>` 标签) 绑定一个 `click` 事件监听器。
        *   在事件处理器中，首先**阻止** `<a>` 标签的默认跳转行为 (`event.preventDefault()`)。
        *   从被点击的卡片上，通过 `data-*` 属性，读取已经存在于页面中的**文章标题、原文链接和 LLM 总结**。
        *   将这些信息动态地填充到隐藏的模态框的对应位置。
        *   使用 Bootstrap 的 JavaScript API，手动触发模态框的显示。

**新方案优势：**
*   **纯静态，零后端**: 完美契合 GitHub Pages。
*   **极致性能**: 用户点击时，总结瞬时加载，无任何网络延迟。
*   **绝对安全**: API 密钥只存在于 Actions 的后端环境中，绝不暴露于前端。
*   **生命周期一致**: 总结与数据一同生成、一同清理。

---

### **第三部分：整合慢更新内容源到 `daily` 侧边栏**

**问题：** 某些内容源（如 `AIBrewsSubstackCrawler`）更新频率较低（例如每周一次），如果直接将其文章混入每日文章流，可能导致 `daily` 页面在大部分时间里没有来自这些源的新内容，侧边栏显得空洞。

**解决方案：独立存储最新文章，并在侧边栏展示**

此方案确保 `daily` 页面的侧边栏能持续展示来自“慢更新”源的最新文章，无论其更新频率如何。

**实现细节：**
1.  **修改 `crawlers/config.json`**:
    *   为需要特殊处理的“慢更新”爬虫（例如 `AIBrewsSubstackCrawler`）添加一个标记，例如 `"is_sidebar_feature": true`。

2.  **修改 `crawlers/main.py`**:
    *   在主循环中，当处理完一个 `is_sidebar_feature: true` 的爬虫后，找到 `articles_with_content` 中日期最新的那篇文章。
    *   将该最新文章的元数据（或其精简版本）保存到一个独立的 JSON 文件中，例如 `_data/latest_{parser_name_snake_case}.json`。
    *   这个 `latest_*.json` 文件每次运行时都会被覆盖，确保它始终包含该源的最新文章。

3.  **修改 `daily.html`**:
    *   在 `daily` 页面的侧边栏部分，添加 Liquid 逻辑。
    *   遍历 `_data` 目录下的所有 `latest_*.json` 文件。
    *   为每个文件中的文章，渲染一个精美的卡片，作为“精选”或“每周亮点”模块进行展示。

**新方案优势：**
*   **内容持续性**: 侧边栏始终有内容展示，即使“慢更新”源当天没有新文章。
*   **信息分层**: 将高频更新和低频但重要的内容进行区分展示。
*   **灵活配置**: 通过 `config.json` 轻松控制哪些源需要这种特殊处理。

---

### **第四部分：待办事项与未来计划**

-   **处理 `favicon.ico` 的 404 错误**: 
    -   **问题**: 浏览器会自动请求网站根目录下的 `favicon.ico` 文件，若不存在会导致 404 错误。
    -   **方案**: 设计一个网站图标，转换为 `.ico` 格式，并放置在项目根目录或 `img/` 目录下，并在 `_includes/head.html` 中通过 `<link>` 标签明确指定路径。

-   **补充 PWA (渐进式 Web 应用) 图标**:
    -   **问题**: `pwa/icons/` 目录下的图标文件已被删除，这会影响“添加到主屏幕”功能的外观。
    -   **方案**: 在未来重新设计并添加 `128x128` 和 `512x512` 像素的 PWA 图标，以完善 PWA 体验。