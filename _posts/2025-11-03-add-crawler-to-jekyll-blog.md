---
layout: post
title: "ä¸ºJekyllåšå®¢é›†æˆä¸€ä¸ªå…¨è‡ªåŠ¨çš„æ¯æ—¥ä¿¡æ¯çˆ¬è™«ï¼ˆä¿å§†çº§æ•™ç¨‹ï¼‰"
subtitle: "ä»Pythonè„šæœ¬åˆ°GitHub Actionsï¼Œå®ç°åŠ¨æ€æŠ“å–ã€ç¼“å­˜ä¸å‰ç«¯å±•ç¤º"
date: 2025-11-03 18:00:00
author: "LH"
tags: [GitHub, Jekyll, Python, Crawler, CI/CD]
group: life
---

## å‰è¨€ï¼šè®©é™æ€åšå®¢â€œæ´»â€èµ·æ¥

Jekyll æ˜¯ä¸€ä¸ªéå¸¸å‡ºè‰²çš„é™æ€ç½‘ç«™ç”Ÿæˆå™¨ï¼Œä½†â€œé™æ€â€äºŒå­—æœ‰æ—¶ä¹Ÿæ„å‘³ç€å†…å®¹çš„æ›´æ–°ä¾èµ–äºæ‰‹åŠ¨çš„ `git push`ã€‚å¦‚æœæˆ‘ä»¬æƒ³è®©åšå®¢çš„æŸä¸ªé¡µé¢èƒ½æ¯å¤©è‡ªåŠ¨æ›´æ–°ï¼Œå±•ç¤ºæ¥è‡ªå…¶ä»–ç½‘ç«™çš„æœ€æ–°ä¿¡æ¯ï¼ˆæ¯”å¦‚æ–°é—»ã€æŠ€æœ¯æ–‡ç« ã€è®ºæ–‡åˆ—è¡¨ç­‰ï¼‰ï¼Œè¯¥æ€ä¹ˆåšå‘¢ï¼Ÿ

è¿™ç¯‡åšæ–‡å°†æ˜¯ä¸€ä»½ä¿å§†çº§çš„å®æˆ˜æ•™ç¨‹ï¼Œè¯¦ç»†è®°å½•æˆ‘ä»¬å¦‚ä½•ä»é›¶å¼€å§‹ï¼Œä¸ºæœ¬åšå®¢é›†æˆä¸€ä¸ªå…¨è‡ªåŠ¨çš„æ¯æ—¥ä¿¡æ¯çˆ¬è™«ç³»ç»Ÿã€‚æˆ‘ä»¬å°†å®ç°ä»¥ä¸‹ç›®æ ‡ï¼š

1.  **è‡ªåŠ¨åŒ–**: åˆ©ç”¨ GitHub Actions å®ç°å®šæ—¶ä»»åŠ¡ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚
2.  **æ¨¡å—åŒ–**: æ„å»ºä¸€ä¸ªå¯æ‰©å±•çš„ Python çˆ¬è™«æ¡†æ¶ï¼Œæ–¹ä¾¿æœªæ¥æ·»åŠ æ›´å¤šçˆ¬å–æºã€‚
3.  **æ™ºèƒ½åŒ–**: å…·å¤‡æŸ¥é‡ã€è‡ªåŠ¨æ¸…ç†è¿‡æœŸæ•°æ®ç­‰åŠŸèƒ½ã€‚
4.  **åŠ¨æ€å±•ç¤º**: å°†çˆ¬å–åˆ°çš„æ•°æ®åŠ¨æ€æ¸²æŸ“åˆ°åšå®¢çš„ "Daily" é¡µé¢ã€‚

è®©æˆ‘ä»¬å¼€å§‹å§ï¼

## ç¬¬ä¸€é˜¶æ®µï¼šè§„åˆ’ä¸åŸºç¡€æ¡†æ¶æ­å»º

å‡¡äº‹é¢„åˆ™ç«‹ï¼Œä¸é¢„åˆ™åºŸã€‚ä¸€ä¸ªæ¸…æ™°çš„è®¡åˆ’æ˜¯æˆåŠŸçš„ä¸€åŠã€‚æˆ‘ä»¬é¦–å…ˆè§„åˆ’äº†æ•´ä¸ªé¡¹ç›®çš„æ¶æ„å’Œæµç¨‹ã€‚

### 1. æœ€ç»ˆç›®æ ‡

æˆ‘ä»¬å¸Œæœ›åœ¨åšå®¢çš„å¯¼èˆªæ å¢åŠ ä¸€ä¸ª "Daily" é¡µé¢ï¼Œè¯¥é¡µé¢æ¯å¤©ä¼šè‡ªåŠ¨æ›´æ–°ï¼Œå±•ç¤ºæˆ‘ä»¬ä»ç‰¹å®šç½‘ç«™ï¼ˆä»¥åšå®¢å›­ç²¾é€‰ä¸ºä¾‹ï¼‰æŠ“å–åˆ°çš„æœ€æ–°æ–‡ç« åˆ—è¡¨ã€‚

### 2. æŠ€æœ¯é€‰å‹

*   **è‡ªåŠ¨åŒ–**: GitHub Actions
*   **çˆ¬è™«æ¡†æ¶**: Python + `crawl4ai` (ä¸€ä¸ªå¯¹LLMå‹å¥½çš„çˆ¬è™«åº“)
*   **æ•°æ®å­˜å‚¨**: æŠ“å–åˆ°çš„å†…å®¹ç¼“å­˜åˆ° `cache/` ç›®å½•ï¼Œå…ƒæ•°æ®ä¿å­˜åˆ° Jekyll èƒ½ç›´æ¥è¯»å–çš„ `_data/` ç›®å½•ã€‚

### 3. æ ¸å¿ƒè®¾è®¡

æˆ‘ä»¬ç¡®å®šäº†å‡ ä¸ªæ ¸å¿ƒçš„è®¾è®¡åŸåˆ™ï¼š

*   **æŒ‰æ—¥æœŸç¼“å­˜**: æ¯å¤©çˆ¬å–çš„å†…å®¹å­˜æ”¾åœ¨ä»¥å½“å¤©æ—¥æœŸå‘½åçš„æ–‡ä»¶å¤¹ä¸­ï¼ˆå¦‚ `cache/2025-11-03/`ï¼‰ï¼Œç»“æ„æ¸…æ™°ã€‚
*   **æ•°æ®å»é‡**: æ¯æ¬¡çˆ¬å–å‰ï¼Œå…ˆåŠ è½½æœ€è¿‘15å¤©å·²çˆ¬å–çš„æ–‡ç« URLï¼Œé¿å…é‡å¤æŠ“å–ã€‚
*   **è‡ªåŠ¨æ¸…ç†**: æ¯æ¬¡ä»»åŠ¡ç»“æŸæ—¶ï¼Œè‡ªåŠ¨åˆ é™¤è¶…è¿‡15å¤©çš„æ—§ç¼“å­˜å’Œæ•°æ®ï¼Œé˜²æ­¢ä»“åº“æ— é™è†¨èƒ€ã€‚
*   **é…ç½®é©±åŠ¨**: è¦çˆ¬å–çš„ç›®æ ‡ç½‘ç«™å’Œè§£æå™¨åç§°ï¼Œéƒ½å®šä¹‰åœ¨ `config.json` ä¸­ï¼Œæ–¹ä¾¿æ‰©å±•ã€‚
*   **é¢å‘å¯¹è±¡**: ä½¿ç”¨â€œåŸºç±»+å­ç±»â€çš„æ¨¡å¼ï¼Œå°†é€šç”¨é€»è¾‘ï¼ˆå¦‚æ–‡ä»¶ä¿å­˜ï¼‰å’Œç‰¹å®šç½‘ç«™çš„è§£æé€»è¾‘è§£è€¦ã€‚

è¿™æ˜¯æˆ‘ä»¬å½“æ—¶ç»˜åˆ¶çš„è“å›¾ï¼Œä¹Ÿæ˜¯æ¥ä¸‹æ¥æ‰€æœ‰æ­¥éª¤çš„ä¾æ®ï¼š

```markdown
# çˆ¬è™«ä¸åšå®¢é›†æˆè®¡åˆ’ (v2)

**åˆ†æ”¯ç­–ç•¥ï¼š**
1.  `feature/crawler-foundation`: æ­å»ºçˆ¬è™«åŸºç¡€æ¡†æ¶ã€‚
2.  `feature/llm-integration`: é›†æˆ LLM API è¿›è¡Œæ€»ç»“ã€‚

---

### ç¬¬ä¸€é˜¶æ®µï¼šçˆ¬è™«åŸºç¡€æ¡†æ¶

1.  **ç¯å¢ƒå‡†å¤‡**: ç¡®å®š Python ç‰ˆæœ¬å’Œä¾èµ–åº“ç‰ˆæœ¬ã€‚
2.  **æ­å»ºç›®å½•ç»“æ„**: åˆ›å»º `crawlers/` å’Œ `llm/` æ–‡ä»¶å¤¹ã€‚
3.  **å®ç°çˆ¬è™«æ¡†æ¶**: ç¼–å†™ `base_crawler.py`, `main.py`, `config.json` å’Œå…·ä½“çš„çˆ¬è™«å­ç±»ã€‚
4.  **é›†æˆåˆ° GitHub Actions**: ä¿®æ”¹ `.github/workflows/deploy.yml`ï¼ŒåŠ å…¥ Python ç¯å¢ƒå’Œçˆ¬è™«è¿è¡Œæ­¥éª¤ã€‚

### ç¬¬äºŒé˜¶æ®µï¼šLLM é›†æˆä¸æ•°æ®å±•ç¤º

...
```

## ç¬¬äºŒé˜¶æ®µï¼šç¼–ç å®ç° - æ‰“é€ çˆ¬è™«æ ¸å¿ƒ

ç°åœ¨ï¼Œæˆ‘ä»¬è¿›å…¥æ¿€åŠ¨äººå¿ƒçš„ç¼–ç ç¯èŠ‚ã€‚æˆ‘ä»¬å°†ä¸€æ­¥æ­¥åˆ›å»ºå‡ºçˆ¬è™«çš„å„ä¸ªæ¨¡å—ã€‚

### 1. ç¯å¢ƒå‡†å¤‡ (`requirements.txt`)

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ª `requirements.txt` æ–‡ä»¶æ¥ç®¡ç†æˆ‘ä»¬çš„Pythonä¾èµ–ã€‚å›ºå®šç‰ˆæœ¬å·æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„ä¹ æƒ¯ï¼Œå®ƒå¯ä»¥ä¿è¯åœ¨ä»»ä½•ç¯å¢ƒä¸‹ï¼ˆåŒ…æ‹¬äº‘ç«¯çš„GitHub Actionsï¼‰å®‰è£…çš„éƒ½æ˜¯ç›¸åŒç‰ˆæœ¬çš„åº“ï¼Œé¿å…å› åº“æ›´æ–°å¯¼è‡´æ„å¤–çš„é”™è¯¯ã€‚

```txt
# requirements.txt

crawl4ai==0.7.6
requests==2.32.5
beautifulsoup4==4.12.2
lxml==5.4.0 # crawl4ai 0.7.6 éœ€è¦ lxml 5.3 ä»¥ä¸Šç‰ˆæœ¬
jsonlines==3.1.0
aiohttp==3.9.5 # å¼‚æ­¥HTTPè¯·æ±‚
```

### 2. çˆ¬è™«åŸºç±» (`crawlers/base_crawler.py`)

æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ª `base_crawler.py` æ–‡ä»¶ã€‚è¿™ä¸ªåŸºç±»çš„ä½œç”¨æ˜¯å°è£…æ‰€æœ‰çˆ¬è™«éƒ½é€šç”¨çš„é€»è¾‘ï¼Œæ¯”å¦‚ä¿å­˜æ–‡æœ¬å†…å®¹ã€å¼‚æ­¥ä¸‹è½½å›¾ç‰‡ç­‰ã€‚å­ç±»åªéœ€è¦ç»§æ‰¿å®ƒï¼Œç„¶åä¸“æ³¨äºå¦‚ä½•è§£æç‰¹å®šç½‘ç«™å³å¯ã€‚

```python
# crawlers/base_crawler.py

from abc import ABC, abstractmethod
import os
import json
import aiohttp

class BaseCrawler(ABC):
    """æ‰€æœ‰çˆ¬è™«çš„æŠ½è±¡åŸºç±»ã€‚"""

    def __init__(self, url, output_dir):
        self.url = url
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)

    async def save_content(self, article_dir, content, images):
        """å°†æ–‡æœ¬å†…å®¹å’Œå›¾ç‰‡ä¿å­˜åˆ°æœ¬åœ°ç›®å½•ã€‚"""
        os.makedirs(article_dir, exist_ok=True)

        # 1. ä¿å­˜æ–‡æœ¬å†…å®¹
        with open(os.path.join(article_dir, "content.txt"), "w", encoding="utf-8") as f:
            f.write(content)

        # 2. å¼‚æ­¥ä¸‹è½½å¹¶ä¿å­˜å›¾ç‰‡
        async with aiohttp.ClientSession() as session:
            for i, img_url in enumerate(images):
                # ç¡®ä¿å›¾ç‰‡URLæ˜¯ç»å¯¹è·¯å¾„
                if not img_url.startswith('http'):
                    img_url = f"https:{img_url}" if img_url.startswith('//') else f"{self.url.rsplit('/', 1)[0]}/{img_url}"
                
                img_path = os.path.join(article_dir, f"image_{i+1}.jpg")
                try:
                    async with session.get(img_url, timeout=10) as response:
                        if response.status == 200:
                            with open(img_path, "wb") as img_file:
                                img_file.write(await response.read())
                        else:
                            print(f"Warning: Failed to download image {img_url} with status {response.status}")
                except Exception as e:
                    print(f"Error: Failed to download image {img_url}: {e}")

    def save_metadata(self, metadata_path, items):
        """å°†å…ƒæ•°æ®ä¿å­˜ä¸ºJSONæ–‡ä»¶ã€‚"""
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(items, f, ensure_ascii=False, indent=4)

    @abstractmethod
    async def crawl(self):
        """çˆ¬å–ç½‘ç«™å¹¶è¿”å›ç»“æ„åŒ–æ•°æ®ã€‚è¿™æ˜¯å­ç±»å¿…é¡»å®ç°çš„æ–¹æ³•ã€‚"""
        pass
```

### 3. åšå®¢å›­çˆ¬è™« (`crawlers/specific_crawlers/cnblogs_crawler.py`)

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ›å»ºç¬¬ä¸€ä¸ªå…·ä½“çš„çˆ¬è™«ï¼Œç”¨äºæŠ“å–åšå®¢å›­ç²¾é€‰æ–‡ç« ã€‚å®ƒç»§æ‰¿è‡ª `BaseCrawler`ï¼Œå¹¶å®ç°äº†å…·ä½“çš„ `crawl` æ–¹æ³•ã€‚

```python
# crawlers/specific_crawlers/cnblogs_crawler.py

from crawl4ai import AsyncWebCrawler
from bs4 import BeautifulSoup
import os
import re
import time
import random
from ..base_crawler import BaseCrawler

def sanitize_filename(filename):
    """ç§»é™¤åœ¨Windows/Linuxæ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦ã€‚"""
    return re.sub(r'[\\/*?"<>|:]', '-', filename)

class CnblogsCrawler(BaseCrawler):
    """åšå®¢å›­çˆ¬è™«ã€‚"""

    def __init__(self, url: str, output_dir: str, existing_urls: set, top_k: int = 5):
        super().__init__(url, output_dir)
        self.top_k = top_k
        self.existing_urls = existing_urls # æ¥æ”¶æ¥è‡ªmain.pyçš„å†å²URLé›†åˆ

    async def crawl(self):
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"}
        crawler = AsyncWebCrawler(headers=headers)
        
        # 1. çˆ¬å–ç²¾é€‰æ–‡ç« åˆ—è¡¨é¡µ
        list_page_result = await crawler.arun(url=self.url)
        if not list_page_result.success:
            print(f"Failed to crawl list page {self.url}: {list_page_result.error_message}")
            return []

        soup = BeautifulSoup(list_page_result.html, "lxml")
        article_links = soup.select("a.post-item-title")[:self.top_k]

        metadata_items = []

        # 2. éå†å¹¶å¤„ç†æ¯ä¸€ç¯‡æ–‡ç« 
        for tag in article_links:
            title = tag.get_text(strip=True)
            link = tag.get("href", "")
            if not link.startswith('http'):
                link = f"https://www.cnblogs.com{link}"

            # æŸ¥é‡ï¼šå¦‚æœæ–‡ç« URLå·²å­˜åœ¨ï¼Œåˆ™è·³è¿‡
            if link in self.existing_urls:
                print(f"Skipping already processed article: {title}")
                continue

            print(f"Processing new article: {title}")

            # è§„èŒƒåŒ–æ–‡ä»¶å
            safe_title = sanitize_filename(title)
            article_dir = os.path.join(self.output_dir, safe_title)

            time.sleep(random.uniform(1, 2)) # ç¤¼è²Œåœ°ç­‰å¾…ä¸€ä¸‹

            # çˆ¬å–æ–‡ç« è¯¦æƒ…é¡µ
            article_result = await crawler.arun(url=link)
            if not article_result.success:
                print(f"Failed to crawl article {link}: {article_result.error_message}")
                continue

            # ä½¿ç”¨crawl4aiè·å–Markdownæ­£æ–‡
            content = article_result.markdown
            
            # ä½¿ç”¨BeautifulSoupåœ¨æ­£æ–‡åŒºåŸŸå†…ç²¾ç¡®æå–å›¾ç‰‡
            article_soup = BeautifulSoup(article_result.html, 'lxml')
            content_body = article_soup.find('div', id='cnblogs_post_body')
            images = []
            if content_body:
                images = [img['src'] for img in content_body.find_all('img') if img.get('src')]
            
            # è°ƒç”¨åŸºç±»çš„ä¿å­˜æ–¹æ³•
            await self.save_content(article_dir, content, images)

            # å‡†å¤‡å…ƒæ•°æ®
            metadata_items.append({
                "title": title,
                "link": link,
                "cache_path": os.path.join(self.output_dir, safe_title)
            })

        return metadata_items
```

### 4. é…ç½®æ–‡ä»¶ (`crawlers/config.json`)

è¿™æ˜¯ä¸€ä¸ªç®€å•çš„JSONæ–‡ä»¶ï¼Œç”¨æ¥å‘Šè¯‰ä¸»ç¨‹åºè¦è¿è¡Œå“ªäº›çˆ¬è™«ã€‚

```json
{
    "sites": [
        {
            "url": "https://www.cnblogs.com/pick/",
            "parser": "CnblogsCrawler"
        }
    ]
}
```

### 5. æ€»ç¼–æ’è„šæœ¬ (`crawlers/main.py`)

è¿™æ˜¯æˆ‘ä»¬çˆ¬è™«ç³»ç»Ÿçš„å¤§è„‘ï¼Œè´Ÿè´£æ‰€æœ‰æµç¨‹çš„ç¼–æ’ï¼šåŠ è½½å†å²ã€è¿è¡Œçˆ¬è™«ã€ä¿å­˜å½“å¤©æ•°æ®ã€æ¸…ç†è¿‡æœŸæ•°æ®ã€‚

```python
# crawlers/main.py

import json
import importlib
import os
import asyncio
import shutil
import re
from datetime import datetime, timedelta
from pathlib import Path

# ... (camel_to_snake, load_existing_urls, cleanup_old_data å‡½æ•°ä»£ç ) ...

async def main():
    """çˆ¬è™«æ€»ç¼–æ’è„šæœ¬"""
    print("Starting crawler orchestration...")
    
    # 1. è®¾ç½®è·¯å¾„å¹¶åŠ è½½å†å²URL
    today = datetime.now().strftime("%Y-%m-%d")
    # ... (è·¯å¾„è®¾ç½®ä»£ç ) ...
    existing_urls = load_existing_urls(data_dir, days_to_keep=15)
    print(f"Found {len(existing_urls)} existing URLs from the last 15 days.")

    # 2. åŠ è½½é…ç½®
    # ... (åŠ è½½ config.json ä»£ç ) ...

    # 3. åŠ¨æ€è¿è¡Œæ‰€æœ‰çˆ¬è™«
    all_metadata = []
    for site in config["sites"]:
        # ... (åŠ¨æ€å¯¼å…¥å¹¶è¿è¡Œçˆ¬è™«çš„ä»£ç ) ...
        # æ³¨å…¥ä¾èµ–ï¼šå½“å¤©çš„ç¼“å­˜ç›®å½•å’Œå†å²URLé›†åˆ
        crawler_instance = CrawlerClass(site["url"], todays_cache_dir, existing_urls)
        metadata = await crawler_instance.crawl()
        all_metadata.extend(metadata)

    # 4. ä¿å­˜å½“å¤©çš„å…ƒæ•°æ®åˆ° _data ç›®å½•
    if all_metadata:
        with open(todays_data_file, 'w', encoding='utf-8') as f:
            json.dump(all_metadata, f, ensure_ascii=False, indent=4)
        print(f"Successfully saved today's metadata to {todays_data_file}")
    else:
        print("No new articles found to save.")

    # 5. æ¸…ç†15å¤©å‰çš„æ—§æ•°æ®
    cleanup_old_data(cache_dir, data_dir, days_to_keep=15)

# ... (ä¸»å‡½æ•°å…¥å£ä»£ç ) ...
```
(æ³¨ï¼šä¸ºç®€æ´èµ·è§ï¼Œ`main.py` ä¸­éƒ¨åˆ†é‡å¤æˆ–è¾…åŠ©å‡½æ•°çš„ä»£ç å·²ç”¨ `...` çœç•¥ï¼Œè¯»è€…å¯å‚è€ƒä»“åº“ä¸­çš„å®Œæ•´æºç ã€‚)*

## ç¬¬ä¸‰é˜¶æ®µï¼šè‡ªåŠ¨åŒ–ä¸å±•ç¤º

ä»£ç å·²ç»å‡†å¤‡å°±ç»ªï¼Œç°åœ¨æˆ‘ä»¬éœ€è¦è®©å®ƒåœ¨äº‘ç«¯è‡ªåŠ¨è¿è¡Œï¼Œå¹¶å°†ç»“æœå±•ç¤ºåœ¨æˆ‘ä»¬çš„åšå®¢ä¸Šã€‚

### 1. é›†æˆåˆ° GitHub Actions (`.github/workflows/deploy.yml`)

æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®©çˆ¬è™«åªåœ¨**å®šæ—¶ä»»åŠ¡**æˆ–**æ‰‹åŠ¨è§¦å‘**æ—¶è¿è¡Œï¼Œè€Œåœ¨æ™®é€šçš„ `git push` æ—¶è·³è¿‡ï¼Œä»¥èŠ‚çœèµ„æºã€‚æˆ‘ä»¬é€šè¿‡ä¿®æ”¹ `deploy.yml` å·¥ä½œæµæ–‡ä»¶æ¥å®ç°è¿™ä¸€ç‚¹ã€‚

```yaml
# .github/workflows/deploy.yml

# ... (çœç•¥äº† on, permissions, concurrency ç­‰è®¾ç½®) ...

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout ğŸ›ï¸
        uses: actions/checkout@v4

      # æ–°å¢ï¼šè®¾ç½® Python 3.11 ç¯å¢ƒ
      - name: Setup Python ğŸ
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      # æ–°å¢ï¼šå®‰è£… Python ä¾èµ–
      - name: Install Python dependencies ğŸ“¦
        run: pip install -r requirements.txt

      # å…³é”®ï¼æ–°å¢ï¼šå®‰è£… Playwright æµè§ˆå™¨å†…æ ¸
      - name: Install Playwright Browsers ğŸ­
        run: playwright install

      # æ–°å¢ï¼šè¿è¡Œçˆ¬è™«è„šæœ¬ï¼Œå¹¶è®¾ç½®è§¦å‘æ¡ä»¶
      - name: Run crawler ğŸ•·ï¸
        if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
        run: python crawlers/main.py

      - name: Setup Ruby and install gems ğŸ’
        # ... (åŸæœ‰çš„ Jekyll è®¾ç½®æ­¥éª¤)

      - name: Build the site ğŸ—ï¸
        # ... (åŸæœ‰çš„ Jekyll æ„å»ºæ­¥éª¤)

      - name: Upload artifact ğŸ“¦
        # ... (åŸæœ‰çš„ä¸Šä¼ æ­¥éª¤)

# ... (çœç•¥äº† deploy job)
```

**æœ€é‡è¦çš„ä¿®æ”¹æœ‰ä¸‰å¤„ï¼š**
1.  **å®‰è£… Python å’Œä¾èµ–**: ä½¿ç”¨ `actions/setup-python` å¹¶è¿è¡Œ `pip install`ã€‚
2.  **å®‰è£… Playwright æµè§ˆå™¨**: è¿™æ˜¯æˆ‘ä»¬åœ¨è°ƒè¯•ä¸­å‘ç°çš„å…³é”®ä¸€æ­¥ã€‚`crawl4ai` åº•å±‚ä½¿ç”¨ `Playwright`ï¼Œå®ƒéœ€è¦ä¸€ä¸ªçœŸå®çš„æµè§ˆå™¨å†…æ ¸æ‰èƒ½å·¥ä½œã€‚`playwright install` å‘½ä»¤ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶å®‰è£…å®ƒã€‚
3.  **æ¡ä»¶åŒ–è¿è¡Œçˆ¬è™«**: `if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'` è¿™è¡Œä»£ç ç¡®ä¿äº†çˆ¬è™«åªåœ¨æˆ‘ä»¬æƒ³è¦å®ƒè¿è¡Œçš„æ—¶å€™è¿è¡Œã€‚

### 2. å‰ç«¯é¡µé¢å±•ç¤º (`daily.html`)

æœ€åä¸€æ­¥ï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹ `daily.html` é¡µé¢ï¼Œè®©å®ƒèƒ½è¯»å– `_data/` ç›®å½•ä¸‹çš„æ•°æ®å¹¶æ¸²æŸ“å‡ºæ¥ã€‚æˆ‘ä»¬ä½¿ç”¨ Jekyll çš„ `Liquid` æ¨¡æ¿è¯­è¨€æ¥å®ç°ã€‚

```html
---
layout: page
title: "Daily"
---

<style>
.post-card { /* ... çœç•¥æ ·å¼ ... */ }
</style>

<div class="container">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            
            {% comment %} 1. æ‰¾åˆ°æœ€æ–°çš„ daily_*.json æ–‡ä»¶ {% endcomment %}
            {% assign latest_date = "1970-01-01" %}
            {% for file in site.data %}
                {% if file[0] contains "daily_" %}
                    {% assign file_date_str = file[0] | remove: "daily_" %}
                    {% if file_date_str > latest_date %}
                        {% assign latest_date = file_date_str %}
                    {% endif %}
                {% endif %}
            {% endfor %}

            {% comment %} 2. è¯»å–å¹¶å±•ç¤ºæœ€æ–°æ–‡ä»¶ä¸­çš„æ–‡ç«  {% endcomment %}
            {% if latest_date != "1970-01-01" %}
                {% assign latest_data_key = "daily_" | append: latest_date %}
                {% assign articles = site.data[latest_data_key] %}

                <p class="post-meta text-center">Showing articles from: {{ latest_date }}</p>
                <hr>

                {% for article in articles %}
                    <a href="{{ article.link }}" target="_blank" class="post-card">
                        <h2>{{ article.title }}</h2>
                        <p class="post-meta">Source: cnblogs</p>
                    </a>
                {% endfor %}
            {% else %}
                <p class="text-center">No daily information available yet. Please run the crawler.</p>
            {% endif %}

        </div>
    </div>
</div>
```
è¿™æ®µä»£ç çš„é€»è¾‘å¾ˆæ¸…æ™°ï¼šé¦–å…ˆéå† `site.data` æ‰¾åˆ°æœ€æ–°çš„æ—¥æœŸï¼Œç„¶ååŠ è½½å¯¹åº”çš„æ•°æ®æ–‡ä»¶ï¼Œæœ€åé€šè¿‡ä¸€ä¸ª `for` å¾ªç¯å°†æ¯ç¯‡æ–‡ç« æ¸²æŸ“æˆä¸€ä¸ªå¯ç‚¹å‡»çš„å¡ç‰‡ã€‚

## ç¬¬å››é˜¶æ®µï¼šè°ƒè¯•ã€æ€»ç»“ä¸æœ€ç»ˆä¼˜åŒ–

åœ¨é¡¹ç›®ä¸Šçº¿åï¼Œæˆ‘ä»¬é‡åˆ°å¹¶è§£å†³äº†ä¸€ç³»åˆ—çœŸå®ä¸–ç•Œä¸­çš„é—®é¢˜ã€‚è¿™äº›è°ƒè¯•æ¡ˆä¾‹æ˜¯æœ¬æ•™ç¨‹æœ€æœ‰ä»·å€¼çš„éƒ¨åˆ†ä¹‹ä¸€ã€‚

### **é™„å½•Aï¼šâ€œè·³è¿‡â€çš„æ–œæ  (/) ä¸æ‰‹åŠ¨è§¦å‘**

**é—®é¢˜ï¼š** `git push` åï¼Œ"Daily" é¡µé¢æ²¡æœ‰æ›´æ–°ï¼ŒActions æ—¥å¿—ä¸­ â€œRun crawlerâ€ æ­¥éª¤æ˜¾ç¤ºä¸ºä¸€ä¸ªç°è‰²çš„æ–œæ  (/)ã€‚

**åŸå› ï¼š** è¿™æ˜¯æˆ‘ä»¬ `if` æ¡ä»¶åˆ¤æ–­æ­£ç¡®ç”Ÿæ•ˆçš„ç»“æœã€‚`push` äº‹ä»¶ä¸æ»¡è¶³ `if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'`ï¼Œå› æ­¤çˆ¬è™«è¢«è·³è¿‡ã€‚

**è§£å†³æ–¹æ¡ˆï¼š** é€šè¿‡ Actions é¡µé¢çš„ "Run workflow" æŒ‰é’®æ‰‹åŠ¨è§¦å‘ä¸€æ¬¡ï¼Œå³å¯æˆåŠŸè¿è¡Œçˆ¬è™«ã€‚

### **é™„å½•Bï¼šå®šæ—¶ä»»åŠ¡çš„æ—¶åŒºé™·é˜±**

**é—®é¢˜ï¼š** è®¾ç½®äº† `cron: '0 8 * * *'`ï¼Œä½†ç¬¬äºŒå¤©æ—©ä¸Š8ç‚¹ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰ä»»åŠ¡æ²¡æœ‰è¿è¡Œã€‚

**åŸå› ï¼š** GitHub Actions çš„ `schedule` ä¸¥æ ¼åŸºäº **UTC æ—¶é—´**ã€‚åŒ—äº¬æ—¶é—´ (UTC+8) çš„æ—©ä¸Š8ç‚¹ï¼Œå¯¹åº”çš„æ˜¯ UTC æ—¶é—´çš„ 0ç‚¹ã€‚

**è§£å†³æ–¹æ¡ˆï¼š** å°† `cron` è¡¨è¾¾å¼ä¿®æ”¹ä¸º `cron: '0 0 * * *'`ã€‚

### **é™„å½•Cï¼šç¼“å­˜çš„ç»ˆæBossâ€”â€”Service Worker**

**é—®é¢˜ï¼š** å³ä½¿éƒ¨ç½²äº†æ–°çš„ UI æ ·å¼ï¼Œåˆ·æ–°æµè§ˆå™¨åçœ‹åˆ°çš„ä¾ç„¶æ˜¯æ—§çš„å¸ƒå±€ã€‚

**åŸå› ï¼š** é¡¹ç›®ä¸­çš„ `sw.js` (Service Worker) é‡‡ç”¨äº†â€œç¼“å­˜ä¼˜å…ˆâ€ç­–ç•¥ï¼Œå®ƒæ‹¦æˆªäº†é¡µé¢è¯·æ±‚å¹¶ç›´æ¥è¿”å›äº†æ—§çš„ã€ç¼“å­˜è¿‡çš„ HTML æ–‡ä»¶ï¼Œå¯¼è‡´æµè§ˆå™¨æ ¹æœ¬æ²¡æœºä¼šåŠ è½½æ–°çš„ CSSã€‚

**è§£å†³æ–¹æ¡ˆï¼š** å°† `sw.js` çš„ç¼“å­˜ç­–ç•¥ä¿®æ”¹ä¸º **â€œç½‘ç»œä¼˜å…ˆ (Network First)â€**ã€‚å³ä¼˜å…ˆè®¿é—®ç½‘ç»œè·å–æœ€æ–°å†…å®¹ï¼Œåªæœ‰åœ¨ç½‘ç»œå¤±è´¥æ—¶æ‰ä½¿ç”¨ç¼“å­˜ã€‚è¿™ä»æ ¹æœ¬ä¸Šä¿è¯äº†ç”¨æˆ·æ€»èƒ½çœ‹åˆ°æœ€æ–°çš„é¡µé¢ã€‚

```javascript
// sw.js (æ ¸å¿ƒé€»è¾‘ç®€åŒ–)
self.addEventListener('fetch', (event) => {
  if (event.request.mode === 'navigate') {
    event.respondWith(
      fetch(event.request) // 1. ä¼˜å…ˆè®¿é—®ç½‘ç»œ
        .then((response) => {
          // 2. æˆåŠŸåˆ™æ›´æ–°ç¼“å­˜
          caches.open(CACHE_NAME).then((cache) => { ... });
          return response;
        })
        .catch(() => {
          // 3. å¤±è´¥åˆ™ä½¿ç”¨ç¼“å­˜
          return caches.match(event.request);
        })
    );
  }
});
```

### æ€»ç»“

é€šè¿‡è¿™æ¬¡å®Œæ•´çš„å®è·µï¼Œæˆ‘ä»¬æˆåŠŸåœ°å°†ä¸€ä¸ªåŠ¨æ€çš„ã€è‡ªåŠ¨åŒ–çš„çˆ¬è™«ç³»ç»Ÿï¼Œæ— ç¼é›†æˆåˆ°äº†ä¸€ä¸ªé™æ€çš„ Jekyll åšå®¢ä¸­ã€‚æˆ‘ä»¬ä¸ä»…å®ç°äº†åŠŸèƒ½ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬ç»å†å¹¶è§£å†³äº†ä¸€ç³»åˆ—åœ¨çœŸå® CI/CD ç¯å¢ƒä¸­å¸¸è§çš„ä¾èµ–é—®é¢˜ã€æ—¶åŒºé—®é¢˜å’Œç¼“å­˜é—®é¢˜ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬çš„åšå®¢æ‹¥æœ‰äº†ä¸€ä¸ªèƒ½è‡ªæˆ‘æ›´æ–°çš„â€œä¿¡æ¯èšåˆå™¨â€ï¼ŒçœŸæ­£åœ°â€œæ´»â€äº†èµ·æ¥ã€‚å¸Œæœ›è¿™ç¯‡è¯¦å°½çš„æ•™ç¨‹èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼

```