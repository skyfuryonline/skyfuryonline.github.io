---
layout: post
title: "ä¸ºJekyllåšå®¢é›†æˆä¸€ä¸ªå…¨è‡ªåŠ¨çš„æ¯æ—¥ä¿¡æ¯çˆ¬è™«ï¼ˆä¿å§†çº§æ•™ç¨‹ï¼‰"
subtitle: "ä»Pythonè„šæœ¬åˆ°GitHub Actionsï¼Œå®ç°åŠ¨æ€æŠ“å–ã€ç¼“å­˜ä¸å‰ç«¯å±•ç¤º"
date: 2025-11-03 18:00:00
author: "LH"
tags: [GitHub, Jekyll, Python, Crawler, CI/CD]
group: life
---

## å‰è¨€ï¼šè®©é™æ€åšå®¢â€œæ´»â€èµ·æ¥

Jekyll æ˜¯ä¸€ä¸ªéå¸¸å‡ºè‰²çš„é™æ€ç½‘ç«™ç”Ÿæˆå™¨ï¼Œä½†â€œé™æ€â€äºŒå­—æœ‰æ—¶ä¹Ÿæ„å‘³ç€å†…å®¹çš„æ›´æ–°ä¾èµ–äºæ‰‹åŠ¨çš„ `git push`ã€‚å¦‚æœæˆ‘ä»¬æƒ³è®©åšå®¢çš„æŸä¸ªé¡µé¢èƒ½æ¯å¤©è‡ªåŠ¨æ›´æ–°ï¼Œå±•ç¤ºæ¥è‡ªå…¶ä»–ç½‘ç«™çš„æœ€æ–°ä¿¡æ¯ï¼ˆæ¯”å¦‚æ–°é—»ã€æŠ€æœ¯æ–‡ç« ã€è®ºæ–‡åˆ—è¡¨ç­‰ï¼‰ï¼Œè¯¥æ€ä¹ˆåšå‘¢ï¼Ÿ

è¿™ç¯‡åšæ–‡å°†æ˜¯ä¸€ä»½ä¿å§†çº§çš„å®æˆ˜æ•™ç¨‹ï¼Œè¯¦ç»†è®°å½•æˆ‘ä»¬å¦‚ä½•ä»é›¶å¼€å§‹ï¼Œä¸ºæœ¬åšå®¢é›†æˆä¸€ä¸ªå…¨è‡ªåŠ¨çš„æ¯æ—¥ä¿¡æ¯çˆ¬è™«ç³»ç»Ÿã€‚æˆ‘ä»¬å°†å®ç°ä»¥ä¸‹ç›®æ ‡ï¼š

1.  **è‡ªåŠ¨åŒ–**: åˆ©ç”¨ GitHub Actions å®ç°å®šæ—¶ä»»åŠ¡ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚
2.  **æ¨¡å—åŒ–**: æ„å»ºä¸€ä¸ªå¯æ‰©å±•çš„ Python çˆ¬è™«æ¡†æ¶ï¼Œæ–¹ä¾¿æœªæ¥æ·»åŠ æ›´å¤šçˆ¬å–æºã€‚
3.  **æ™ºèƒ½åŒ–**: å…·å¤‡æŸ¥é‡ã€è‡ªåŠ¨æ¸…ç†è¿‡æœŸæ•°æ®ç­‰åŠŸèƒ½ã€‚
4.  **åŠ¨æ€å±•ç¤º**: å°†çˆ¬å–åˆ°çš„æ•°æ®åŠ¨æ€æ¸²æŸ“åˆ°åšå®¢çš„ "Daily" é¡µé¢ã€‚

è®©æˆ‘ä»¬å¼€å§‹å§ï¼

## ç¬¬ä¸€é˜¶æ®µï¼šè§„åˆ’ä¸åŸºç¡€æ¡†æ¶æ­å»º

å‡¡äº‹é¢„åˆ™ç«‹ï¼Œä¸é¢„åˆ™åºŸã€‚ä¸€ä¸ªæ¸…æ™°çš„è®¡åˆ’æ˜¯æˆåŠŸçš„ä¸€åŠã€‚æˆ‘ä»¬é¦–å…ˆè§„åˆ’äº†æ•´ä¸ªé¡¹ç›®çš„æ¶æ„å’Œæµç¨‹ã€‚

### 1. æœ€ç»ˆç›®æ ‡

æˆ‘ä»¬å¸Œæœ›åœ¨åšå®¢çš„å¯¼èˆªæ å¢åŠ ä¸€ä¸ª "Daily" é¡µé¢ï¼Œè¯¥é¡µé¢æ¯å¤©ä¼šè‡ªåŠ¨æ›´æ–°ï¼Œå±•ç¤ºæˆ‘ä»¬ä»ç‰¹å®šç½‘ç«™ï¼ˆä»¥åšå®¢å›­ç²¾é€‰ä¸ºä¾‹ï¼‰æŠ“å–åˆ°çš„æœ€æ–°æ–‡ç« åˆ—è¡¨ã€‚

### 2. æŠ€æœ¯é€‰å‹

*   **è‡ªåŠ¨åŒ–**: GitHub Actions
*   **çˆ¬è™«æ¡†æ¶**: Python + `crawl4ai` (ä¸€ä¸ªå¯¹LLMå‹å¥½çš„çˆ¬è™«åº“)
*   **æ•°æ®å­˜å‚¨**: æŠ“å–åˆ°çš„å†…å®¹ç¼“å­˜åˆ° `cache/` ç›®å½•ï¼Œå…ƒæ•°æ®ä¿å­˜åˆ° Jekyll èƒ½ç›´æ¥è¯»å–çš„ `_data/` ç›®å½•ã€‚

### 3. æ ¸å¿ƒè®¾è®¡

æˆ‘ä»¬ç¡®å®šäº†å‡ ä¸ªæ ¸å¿ƒçš„è®¾è®¡åŸåˆ™ï¼š

*   **æŒ‰æ—¥æœŸç¼“å­˜**: æ¯å¤©çˆ¬å–çš„å†…å®¹å­˜æ”¾åœ¨ä»¥å½“å¤©æ—¥æœŸå‘½åçš„æ–‡ä»¶å¤¹ä¸­ï¼ˆå¦‚ `cache/2025-11-03/`ï¼‰ï¼Œç»“æ„æ¸…æ™°ã€‚
*   **æ•°æ®å»é‡**: æ¯æ¬¡çˆ¬å–å‰ï¼Œå…ˆåŠ è½½æœ€è¿‘15å¤©å·²çˆ¬å–çš„æ–‡ç« URLï¼Œé¿å…é‡å¤æŠ“å–ã€‚
*   **è‡ªåŠ¨æ¸…ç†**: æ¯æ¬¡ä»»åŠ¡ç»“æŸæ—¶ï¼Œè‡ªåŠ¨åˆ é™¤è¶…è¿‡15å¤©çš„æ—§ç¼“å­˜å’Œæ•°æ®ï¼Œé˜²æ­¢ä»“åº“æ— é™è†¨èƒ€ã€‚
*   **é…ç½®é©±åŠ¨**: è¦çˆ¬å–çš„ç›®æ ‡ç½‘ç«™å’Œè§£æå™¨åç§°ï¼Œéƒ½å®šä¹‰åœ¨ `config.json` ä¸­ï¼Œæ–¹ä¾¿æ‰©å±•ã€‚
*   **é¢å‘å¯¹è±¡**: ä½¿ç”¨â€œåŸºç±»+å­ç±»â€çš„æ¨¡å¼ï¼Œå°†é€šç”¨é€»è¾‘ï¼ˆå¦‚æ–‡ä»¶ä¿å­˜ï¼‰å’Œç‰¹å®šç½‘ç«™çš„è§£æé€»è¾‘è§£è€¦ã€‚

è¿™æ˜¯æˆ‘ä»¬å½“æ—¶ç»˜åˆ¶çš„è“å›¾ï¼Œä¹Ÿæ˜¯æ¥ä¸‹æ¥æ‰€æœ‰æ­¥éª¤çš„ä¾æ®ï¼š

```markdown
# çˆ¬è™«ä¸åšå®¢é›†æˆè®¡åˆ’ (v2)

**åˆ†æ”¯ç­–ç•¥ï¼š**
1.  `feature/crawler-foundation`: æ­å»ºçˆ¬è™«åŸºç¡€æ¡†æ¶ã€‚
2.  `feature/llm-integration`: é›†æˆ LLM API è¿›è¡Œæ€»ç»“ã€‚

---

### ç¬¬ä¸€é˜¶æ®µï¼šçˆ¬è™«åŸºç¡€æ¡†æ¶

1.  **ç¯å¢ƒå‡†å¤‡**: ç¡®å®š Python ç‰ˆæœ¬å’Œä¾èµ–åº“ç‰ˆæœ¬ã€‚
2.  **æ­å»ºç›®å½•ç»“æ„**: åˆ›å»º `crawlers/` å’Œ `llm/` æ–‡ä»¶å¤¹ã€‚
3.  **å®ç°çˆ¬è™«æ¡†æ¶**: ç¼–å†™ `base_crawler.py`, `main.py`, `config.json` å’Œå…·ä½“çš„çˆ¬è™«å­ç±»ã€‚
4.  **é›†æˆåˆ° GitHub Actions**: ä¿®æ”¹ `.github/workflows/deploy.yml`ï¼ŒåŠ å…¥ Python ç¯å¢ƒå’Œçˆ¬è™«è¿è¡Œæ­¥éª¤ã€‚

### ç¬¬äºŒé˜¶æ®µï¼šLLM é›†æˆä¸æ•°æ®å±•ç¤º

...
```

## ç¬¬äºŒé˜¶æ®µï¼šç¼–ç å®ç° - æ‰“é€ çˆ¬è™«æ ¸å¿ƒ

ç°åœ¨ï¼Œæˆ‘ä»¬è¿›å…¥æ¿€åŠ¨äººå¿ƒçš„ç¼–ç ç¯èŠ‚ã€‚æˆ‘ä»¬å°†ä¸€æ­¥æ­¥åˆ›å»ºå‡ºçˆ¬è™«çš„å„ä¸ªæ¨¡å—ã€‚

### 1. ç¯å¢ƒå‡†å¤‡ (`requirements.txt`)

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ª `requirements.txt` æ–‡ä»¶æ¥ç®¡ç†æˆ‘ä»¬çš„Pythonä¾èµ–ã€‚å›ºå®šç‰ˆæœ¬å·æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„ä¹ æƒ¯ï¼Œå®ƒå¯ä»¥ä¿è¯åœ¨ä»»ä½•ç¯å¢ƒä¸‹ï¼ˆåŒ…æ‹¬äº‘ç«¯çš„GitHub Actionsï¼‰å®‰è£…çš„éƒ½æ˜¯ç›¸åŒç‰ˆæœ¬çš„åº“ï¼Œé¿å…å› åº“æ›´æ–°å¯¼è‡´æ„å¤–çš„é”™è¯¯ã€‚

```txt
# requirements.txt

crawl4ai==0.7.6
requests==2.32.5
beautifulsoup4==4.12.2
lxml==5.4.0 # crawl4ai 0.7.6 éœ€è¦ lxml 5.3 ä»¥ä¸Šç‰ˆæœ¬
jsonlines==3.1.0
aiohttp==3.9.5 # å¼‚æ­¥HTTPè¯·æ±‚
```

### 2. çˆ¬è™«åŸºç±» (`crawlers/base_crawler.py`)

æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ª `base_crawler.py` æ–‡ä»¶ã€‚è¿™ä¸ªåŸºç±»çš„ä½œç”¨æ˜¯å°è£…æ‰€æœ‰çˆ¬è™«éƒ½é€šç”¨çš„é€»è¾‘ï¼Œæ¯”å¦‚ä¿å­˜æ–‡æœ¬å†…å®¹ã€å¼‚æ­¥ä¸‹è½½å›¾ç‰‡ç­‰ã€‚å­ç±»åªéœ€è¦ç»§æ‰¿å®ƒï¼Œç„¶åä¸“æ³¨äºå¦‚ä½•è§£æç‰¹å®šç½‘ç«™å³å¯ã€‚

```python
# crawlers/base_crawler.py

from abc import ABC, abstractmethod
import os
import json
import aiohttp

class BaseCrawler(ABC):
    """æ‰€æœ‰çˆ¬è™«çš„æŠ½è±¡åŸºç±»ã€‚"""

    def __init__(self, url, output_dir):
        self.url = url
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)

    async def save_content(self, article_dir, content, images):
        """å°†æ–‡æœ¬å†…å®¹å’Œå›¾ç‰‡ä¿å­˜åˆ°æœ¬åœ°ç›®å½•ã€‚"""
        os.makedirs(article_dir, exist_ok=True)

        # 1. ä¿å­˜æ–‡æœ¬å†…å®¹
        with open(os.path.join(article_dir, "content.txt"), "w", encoding="utf-8") as f:
            f.write(content)

        # 2. å¼‚æ­¥ä¸‹è½½å¹¶ä¿å­˜å›¾ç‰‡
        async with aiohttp.ClientSession() as session:
            for i, img_url in enumerate(images):
                # ç¡®ä¿å›¾ç‰‡URLæ˜¯ç»å¯¹è·¯å¾„
                if not img_url.startswith('http'):
                    img_url = f"https:{img_url}" if img_url.startswith('//') else f"{self.url.rsplit('/', 1)[0]}/{img_url}"
                
                img_path = os.path.join(article_dir, f"image_{i+1}.jpg")
                try:
                    async with session.get(img_url, timeout=10) as response:
                        if response.status == 200:
                            with open(img_path, "wb") as img_file:
                                img_file.write(await response.read())
                        else:
                            print(f"Warning: Failed to download image {img_url} with status {response.status}")
                except Exception as e:
                    print(f"Error: Failed to download image {img_url}: {e}")

    def save_metadata(self, metadata_path, items):
        """å°†å…ƒæ•°æ®ä¿å­˜ä¸ºJSONæ–‡ä»¶ã€‚"""
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(items, f, ensure_ascii=False, indent=4)

    @abstractmethod
    async def crawl(self):
        """çˆ¬å–ç½‘ç«™å¹¶è¿”å›ç»“æ„åŒ–æ•°æ®ã€‚è¿™æ˜¯å­ç±»å¿…é¡»å®ç°çš„æ–¹æ³•ã€‚"""
        pass
```

### 3. åšå®¢å›­çˆ¬è™« (`crawlers/specific_crawlers/cnblogs_crawler.py`)

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ›å»ºç¬¬ä¸€ä¸ªå…·ä½“çš„çˆ¬è™«ï¼Œç”¨äºæŠ“å–åšå®¢å›­ç²¾é€‰æ–‡ç« ã€‚å®ƒç»§æ‰¿è‡ª `BaseCrawler`ï¼Œå¹¶å®ç°äº†å…·ä½“çš„ `crawl` æ–¹æ³•ã€‚

```python
# crawlers/specific_crawlers/cnblogs_crawler.py

from crawl4ai import AsyncWebCrawler
from bs4 import BeautifulSoup
import os
import re
import time
import random
from ..base_crawler import BaseCrawler

def sanitize_filename(filename):
    """ç§»é™¤åœ¨Windows/Linuxæ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦ã€‚"""
    return re.sub(r'[\\/*?"<>|:]', '-', filename)

class CnblogsCrawler(BaseCrawler):
    """åšå®¢å›­çˆ¬è™«ã€‚"""

    def __init__(self, url: str, output_dir: str, existing_urls: set, top_k: int = 5):
        super().__init__(url, output_dir)
        self.top_k = top_k
        self.existing_urls = existing_urls # æ¥æ”¶æ¥è‡ªmain.pyçš„å†å²URLé›†åˆ

    async def crawl(self):
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"}
        crawler = AsyncWebCrawler(headers=headers)
        
        # 1. çˆ¬å–ç²¾é€‰æ–‡ç« åˆ—è¡¨é¡µ
        list_page_result = await crawler.arun(url=self.url)
        if not list_page_result.success:
            print(f"Failed to crawl list page {self.url}: {list_page_result.error_message}")
            return []

        soup = BeautifulSoup(list_page_result.html, "lxml")
        article_links = soup.select("a.post-item-title")[:self.top_k]

        metadata_items = []

        # 2. éå†å¹¶å¤„ç†æ¯ä¸€ç¯‡æ–‡ç« 
        for tag in article_links:
            title = tag.get_text(strip=True)
            link = tag.get("href", "")
            if not link.startswith('http'):
                link = f"https://www.cnblogs.com{link}"

            # æŸ¥é‡ï¼šå¦‚æœæ–‡ç« URLå·²å­˜åœ¨ï¼Œåˆ™è·³è¿‡
            if link in self.existing_urls:
                print(f"Skipping already processed article: {title}")
                continue

            print(f"Processing new article: {title}")

            # è§„èŒƒåŒ–æ–‡ä»¶å
            safe_title = sanitize_filename(title)
            article_dir = os.path.join(self.output_dir, safe_title)

            time.sleep(random.uniform(1, 2)) # ç¤¼è²Œåœ°ç­‰å¾…ä¸€ä¸‹

            # çˆ¬å–æ–‡ç« è¯¦æƒ…é¡µ
            article_result = await crawler.arun(url=link)
            if not article_result.success:
                print(f"Failed to crawl article {link}: {article_result.error_message}")
                continue

            # ä½¿ç”¨crawl4aiè·å–Markdownæ­£æ–‡
            content = article_result.markdown
            
            # ä½¿ç”¨BeautifulSoupåœ¨æ­£æ–‡åŒºåŸŸå†…ç²¾ç¡®æå–å›¾ç‰‡
            article_soup = BeautifulSoup(article_result.html, 'lxml')
            content_body = article_soup.find('div', id='cnblogs_post_body')
            images = []
            if content_body:
                images = [img['src'] for img in content_body.find_all('img') if img.get('src')]
            
            # è°ƒç”¨åŸºç±»çš„ä¿å­˜æ–¹æ³•
            await self.save_content(article_dir, content, images)

            # å‡†å¤‡å…ƒæ•°æ®
            metadata_items.append({
                "title": title,
                "link": link,
                "cache_path": os.path.join(self.output_dir, safe_title)
            })

        return metadata_items
```

### 4. é…ç½®æ–‡ä»¶ (`crawlers/config.json`)

è¿™æ˜¯ä¸€ä¸ªç®€å•çš„JSONæ–‡ä»¶ï¼Œç”¨æ¥å‘Šè¯‰ä¸»ç¨‹åºè¦è¿è¡Œå“ªäº›çˆ¬è™«ã€‚

```json
{
    "sites": [
        {
            "url": "https://www.cnblogs.com/pick/",
            "parser": "CnblogsCrawler"
        }
    ]
}
```

### 5. æ€»ç¼–æ’è„šæœ¬ (`crawlers/main.py`)

è¿™æ˜¯æˆ‘ä»¬çˆ¬è™«ç³»ç»Ÿçš„å¤§è„‘ï¼Œè´Ÿè´£æ‰€æœ‰æµç¨‹çš„ç¼–æ’ï¼šåŠ è½½å†å²ã€è¿è¡Œçˆ¬è™«ã€ä¿å­˜å½“å¤©æ•°æ®ã€æ¸…ç†è¿‡æœŸæ•°æ®ã€‚

```python
# crawlers/main.py

import json
import importlib
import os
import asyncio
import shutil
import re
from datetime import datetime, timedelta
from pathlib import Path

# ... (camel_to_snake, load_existing_urls, cleanup_old_data å‡½æ•°ä»£ç ) ...

async def main():
    """çˆ¬è™«æ€»ç¼–æ’è„šæœ¬"""
    print("Starting crawler orchestration...")
    
    # 1. è®¾ç½®è·¯å¾„å¹¶åŠ è½½å†å²URL
    today = datetime.now().strftime("%Y-%m-%d")
    # ... (è·¯å¾„è®¾ç½®ä»£ç ) ...
    existing_urls = load_existing_urls(data_dir, days_to_keep=15)
    print(f"Found {len(existing_urls)} existing URLs from the last 15 days.")

    # 2. åŠ è½½é…ç½®
    # ... (åŠ è½½ config.json ä»£ç ) ...

    # 3. åŠ¨æ€è¿è¡Œæ‰€æœ‰çˆ¬è™«
    all_metadata = []
    for site in config["sites"]:
        # ... (åŠ¨æ€å¯¼å…¥å¹¶è¿è¡Œçˆ¬è™«çš„ä»£ç ) ...
        # æ³¨å…¥ä¾èµ–ï¼šå½“å¤©çš„ç¼“å­˜ç›®å½•å’Œå†å²URLé›†åˆ
        crawler_instance = CrawlerClass(site["url"], todays_cache_dir, existing_urls)
        metadata = await crawler_instance.crawl()
        all_metadata.extend(metadata)

    # 4. ä¿å­˜å½“å¤©çš„å…ƒæ•°æ®åˆ° _data ç›®å½•
    if all_metadata:
        with open(todays_data_file, 'w', encoding='utf-8') as f:
            json.dump(all_metadata, f, ensure_ascii=False, indent=4)
        print(f"Successfully saved today's metadata to {todays_data_file}")
    else:
        print("No new articles found to save.")

    # 5. æ¸…ç†15å¤©å‰çš„æ—§æ•°æ®
    cleanup_old_data(cache_dir, data_dir, days_to_keep=15)

# ... (ä¸»å‡½æ•°å…¥å£ä»£ç ) ...
## ç¬¬å››é˜¶æ®µï¼šè°ƒè¯•ä¸æ€»ç»“

åœ¨æˆ‘ä»¬å°†æ‰€æœ‰ä»£ç åˆå¹¶åˆ° `master` åˆ†æ”¯åï¼Œæˆ‘ä»¬é‡åˆ°äº†ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼š**ä»£ç æ¨é€åï¼Œ"Daily" é¡µé¢ä¸Šå¹¶æ²¡æœ‰ç«‹åˆ»å‡ºç°å†…å®¹ï¼Œå¹¶ä¸”åœ¨ GitHub Actions çš„æ—¥å¿—ä¸­ï¼Œâ€œRun crawlerâ€ æ­¥éª¤çš„å›¾æ ‡æ˜¯ä¸€ä¸ªç°è‰²çš„æ–œæ  (/)ï¼Œè€Œä¸æ˜¯ç»¿è‰²çš„å‹¾ (âœ“)ã€‚**

![](/img/crawler-log-skipped.png) *(è¿™é‡Œå¯ä»¥æ”¾ä¸€å¼ GitHub Actionsæ—¥å¿—æˆªå›¾)*

### â€œè·³è¿‡â€çš„æ–œæ  (/) æ˜¯æ€ä¹ˆå›äº‹ï¼Ÿ

è¿™å…¶å®æ˜¯**å®Œå…¨ç¬¦åˆæˆ‘ä»¬é¢„æœŸ**çš„æ­£å¸¸ç°è±¡ï¼

åœ¨ GitHub Actions çš„æ—¥å¿—ä¸­ï¼Œä¸åŒå›¾æ ‡çš„å«ä¹‰æ˜¯ï¼š
*   **ç»¿è‰²çš„å‹¾ (âœ“)**: æ­¥éª¤**æˆåŠŸè¿è¡Œ**ã€‚
*   **çº¢è‰²çš„å‰ (âœ—)**: æ­¥éª¤**è¿è¡Œå¤±è´¥**ã€‚
*   **ç°è‰²çš„æ–œæ  (/)**: æ­¥éª¤è¢«**è·³è¿‡ (Skipped)**ã€‚

æˆ‘ä»¬ä¹‹å‰åœ¨ `deploy.yml` ä¸­ä¸ºçˆ¬è™«æ­¥éª¤è®¾ç½®äº† `if` æ¡ä»¶ï¼Œå¯¼è‡´å®ƒåªåœ¨å®šæ—¶æˆ–æ‰‹åŠ¨è§¦å‘æ—¶è¿è¡Œã€‚ç”±äºæˆ‘ä»¬æ˜¯é€šè¿‡ `git push` è§¦å‘çš„å·¥ä½œæµï¼Œ`github.event_name` çš„å€¼æ˜¯ `push`ï¼Œä¸æ»¡è¶³æ¡ä»¶ï¼Œæ‰€ä»¥ Actions **æ­£ç¡®åœ°è·³è¿‡äº†**è¿™ä¸ªæ­¥éª¤ã€‚è‡ªç„¶ï¼Œé¡µé¢ä¸Šä¹Ÿå°±ä¸ä¼šæœ‰æ–°æ•°æ®äº†ã€‚

### å¦‚ä½•çœ‹åˆ°æœ€ç»ˆæ•ˆæœï¼Ÿ

ä¸ºäº†çœŸæ­£è¿è¡Œçˆ¬è™«å¹¶çœ‹åˆ°æ•ˆæœï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨è§¦å‘ä¸€æ¬¡å·¥ä½œæµï¼š
1.  æ‰“å¼€ GitHub ä»“åº“é¡µé¢ï¼Œç‚¹å‡» **"Actions"** æ ‡ç­¾ã€‚
2.  åœ¨å·¦ä¾§é€‰æ‹©æˆ‘ä»¬çš„å·¥ä½œæµï¼ˆä¾‹å¦‚ "Jekyll site CI"ï¼‰ã€‚
3.  ç‚¹å‡» **"Run workflow"** ä¸‹æ‹‰æŒ‰é’®ï¼Œå†æ¬¡ç¡®è®¤è¿è¡Œã€‚

æ‰‹åŠ¨è§¦å‘åï¼Œæˆ‘ä»¬å°±èƒ½åœ¨æ—¥å¿—ä¸­çœ‹åˆ° â€œRun crawlerâ€ æ­¥éª¤è¢«æˆåŠŸæ‰§è¡Œï¼ˆç»¿è‰²çš„å‹¾ âœ“ï¼‰ï¼Œç¨ç­‰ç‰‡åˆ»ï¼Œè®¿é—®æˆ‘ä»¬çš„åšå®¢ï¼Œ"Daily" é¡µé¢ä¸Šå°±ä¼šå‡ºç°ä»åšå®¢å›­æŠ“å–åˆ°çš„æœ€æ–°æ–‡ç« åˆ—è¡¨äº†ï¼

### æ€»ç»“

é€šè¿‡è¿™æ¬¡å®è·µï¼Œæˆ‘ä»¬æˆåŠŸåœ°å°†ä¸€ä¸ªåŠ¨æ€çš„ã€è‡ªåŠ¨åŒ–çš„çˆ¬è™«ç³»ç»Ÿï¼Œæ— ç¼é›†æˆåˆ°äº†ä¸€ä¸ªé™æ€çš„ Jekyll åšå®¢ä¸­ã€‚æˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•ï¼š

*   ä½¿ç”¨ Python æ„å»ºä¸€ä¸ªæ¨¡å—åŒ–ã€å¯æ‰©å±•çš„çˆ¬è™«æ¡†æ¶ã€‚
*   åˆ©ç”¨ GitHub Actions çš„ `schedule` å’Œ `if` æ¡ä»¶ï¼Œå®ç°ç²¾ç¡®çš„è‡ªåŠ¨åŒ–ä»»åŠ¡æ§åˆ¶ã€‚
*   è§£å†³åœ¨ CI/CD ç¯å¢ƒä¸­ç”± `Playwright` ç­‰å¤æ‚å·¥å…·é“¾å¸¦æ¥çš„ä¾èµ–é—®é¢˜ã€‚
*   é€šè¿‡ Jekyll çš„ `_data` ç›®å½•å’Œ `Liquid` æ¨¡æ¿è¯­è¨€ï¼Œå°†åŠ¨æ€æ•°æ®æ¸²æŸ“åˆ°é™æ€é¡µé¢ä¸Šã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬çš„åšå®¢æ‹¥æœ‰äº†ä¸€ä¸ªèƒ½è‡ªæˆ‘æ›´æ–°çš„â€œä¿¡æ¯èšåˆå™¨â€ï¼ŒçœŸæ­£åœ°â€œæ´»â€äº†èµ·æ¥ã€‚å¸Œæœ›è¿™ç¯‡è¯¦ç»†çš„æ•™ç¨‹èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼

---

### **é™„å½•ï¼šåˆä¸€ä¸ªè°ƒè¯•æ¡ˆä¾‹â€”â€”å®šæ—¶ä»»åŠ¡ä¸ºä»€ä¹ˆæ²¡åœ¨â€œæ—©ä¸Š8ç‚¹â€è¿è¡Œï¼Ÿ**

åœ¨æˆ‘ä»¬å°†æ‰€æœ‰åŠŸèƒ½éƒ½éƒ¨ç½²å¥½ä¹‹åï¼Œç¬¬äºŒå¤©æ—©ä¸Šï¼Œæˆ‘ä»¬å‘ç° "Daily" é¡µé¢å¹¶æ²¡æœ‰æ›´æ–°ã€‚æˆ‘ä»¬æ£€æŸ¥äº† Actions çš„è¿è¡Œè®°å½•ï¼Œå‘ç°å¹¶æ²¡æœ‰åœ¨æ—©ä¸Š8ç‚¹ç”± `schedule` è§¦å‘çš„è®°å½•ã€‚

**é—®é¢˜æ ¹æºï¼šæ—¶åŒºï¼**

è¿™æ˜¯ CI/CD ä¸­ä¸€ä¸ªæå…¶å¸¸è§çš„é™·é˜±ã€‚æˆ‘ä»¬åœ¨ `deploy.yml` ä¸­å†™çš„ `cron: '0 8 * * *'`ï¼Œç›´è§‚åœ°ç†è§£æ˜¯â€œæ¯å¤©æ—©ä¸Š8ç‚¹â€ã€‚ä½†é—®é¢˜æ˜¯ï¼Œè¿™æ˜¯å“ªä¸ªæ—¶åŒºçš„æ—©ä¸Š8ç‚¹ï¼Ÿ

**GitHub Actions çš„æ‰€æœ‰ `schedule` äº‹ä»¶ï¼Œéƒ½ä¸¥æ ¼åŸºäº UTC (åè°ƒä¸–ç•Œæ—¶) æ—¶é—´ã€‚**

*   æˆ‘ä»¬æœŸæœ›çš„æ˜¯ **åŒ—äº¬æ—¶é—´ (CST)** çš„æ—©ä¸Š 8 ç‚¹ã€‚
*   åŒ—äº¬æ—¶é—´æ¯” UTC æ—¶é—´å¿« 8 ä¸ªå°æ—¶ (UTC+8)ã€‚
*   å› æ­¤ï¼Œå½“åŒ—äº¬æ—¶é—´æ˜¯æ—©ä¸Š 8:00 æ—¶ï¼ŒUTC æ—¶é—´å…¶å®æ˜¯ 0:00ã€‚

æ‰€ä»¥ï¼Œæˆ‘ä»¬è®¾ç½®çš„ `0 8 * * *` (UTC 8:00)ï¼Œå®é™…ä¸Šä¼šåœ¨åŒ—äº¬æ—¶é—´ä¸‹åˆ 16:00 (ä¸‹åˆ4ç‚¹) æ‰ä¼šè¿è¡Œã€‚

**è§£å†³æ–¹æ¡ˆï¼š**

å°† `cron` è¡¨è¾¾å¼è°ƒæ•´ä¸ºæ­£ç¡®çš„ UTC æ—¶é—´å³å¯ã€‚

```yaml
  schedule:
    - cron: '0 0 * * *' # å¯¹åº” UTC 0:00ï¼Œå³åŒ—äº¬æ—¶é—´ 8:00
```

è¿™ä¸ªå°å°çš„æ¡ˆä¾‹æé†’æˆ‘ä»¬ï¼Œåœ¨å¤„ç†ä»»ä½•ä¸â€œæ—¶é—´â€ç›¸å…³çš„è‡ªåŠ¨åŒ–ä»»åŠ¡æ—¶ï¼Œæ°¸è¿œè¦ç¬¬ä¸€æ—¶é—´ç¡®è®¤å…¶åŸºå‡†æ—¶åŒºï¼Œè¿™èƒ½ä¸ºæˆ‘ä»¬çœä¸‹å¤§é‡çš„è°ƒè¯•æ—¶é—´ã€‚

---

### **é™„å½•Bï¼šæœ€ç»ˆä¼˜åŒ–â€”â€”ä¸ºä»€ä¹ˆä¿®æ”¹äº†UIï¼Œåˆ·æ–°åå´çœ‹ä¸åˆ°ï¼Ÿ**

åœ¨æˆ‘ä»¬å®Œæˆäº†æ‰€æœ‰åŠŸèƒ½ï¼Œå¹¶å¯¹ "Daily" é¡µé¢çš„ UI è¿›è¡Œäº†ç²¾ç»†çš„è°ƒæ•´åï¼Œæˆ‘ä»¬é‡åˆ°äº†æœ€åä¸€ä¸ªï¼Œä¹Ÿæ˜¯æœ€ç»å…¸çš„ä¸€ä¸ªå‰ç«¯é—®é¢˜ï¼š

**â€œæˆ‘æ˜æ˜å·²ç»æŠŠæœ€æ–°çš„ä»£ç éƒ¨ç½²ä¸Šå»äº†ï¼Œä¸ºä»€ä¹ˆåˆ·æ–°é¡µé¢åï¼Œçœ‹åˆ°çš„è¿˜æ˜¯æ—§çš„æ ·å¼ï¼Ÿâ€**

å³ä½¿ç”¨æˆ·å¼ºåˆ¶åˆ·æ–°ï¼ˆ`Ctrl+Shift+R`ï¼‰ï¼Œæœ‰æ—¶ä¹Ÿæ— æ³•ç«‹åˆ»çœ‹åˆ°æœ€æ–°çš„å¸ƒå±€ã€‚è¿™å¯¹äºç”¨æˆ·ä½“éªŒæ¥è¯´æ˜¯ä¸å¯æ¥å—çš„ã€‚

**é—®é¢˜æ ¹æºï¼šæµè§ˆå™¨ç¼“å­˜ (Browser Caching)**

ä¸ºäº†æé«˜ç½‘ç«™çš„åŠ è½½é€Ÿåº¦ï¼Œæµè§ˆå™¨ä¼šæŠŠå·²ç»ä¸‹è½½è¿‡çš„é™æ€èµ„æºï¼ˆæ¯”å¦‚ CSS æ ·å¼è¡¨å’Œ JS è„šæœ¬ï¼‰ä¿å­˜åœ¨æœ¬åœ°ã€‚å½“ç”¨æˆ·å†æ¬¡è®¿é—®æ—¶ï¼Œæµè§ˆå™¨ä¼šç›´æ¥ä½¿ç”¨è¿™äº›æœ¬åœ°çš„æ—§æ–‡ä»¶ï¼Œè€Œä¸æ˜¯å»æœåŠ¡å™¨è¯·æ±‚æœ€æ–°çš„ç‰ˆæœ¬ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éƒ¨ç½²äº†æ–°çš„ CSSï¼Œä½†æµè§ˆå™¨ä¾ç„¶åœ¨ä½¿ç”¨æ—§çš„æ ·å¼ã€‚

**è§£å†³æ–¹æ¡ˆï¼šç¼“å­˜ç ´å (Cache Busting)**

æˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹æ³•æ¥â€œæ¬ºéª—â€æµè§ˆå™¨ï¼Œè®©å®ƒè®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„æ–‡ä»¶ï¼Œä»è€Œä¸»åŠ¨å»ä¸‹è½½ã€‚æœ€ç®€å•ã€æœ€æœ‰æ•ˆçš„å®ç°æ–¹å¼æ˜¯ï¼š**ä¸ºé™æ€èµ„æºçš„ URL æ·»åŠ ä¸€ä¸ªæ¯æ¬¡æ„å»ºéƒ½ä¼šæ”¹å˜çš„æŸ¥è¯¢å­—ç¬¦ä¸²ã€‚**

æˆ‘ä»¬åœ¨ `_includes/head.html` ä¸­æ‰¾åˆ°äº†å¼•ç”¨ä¸»æ ·å¼è¡¨çš„åœ°æ–¹ï¼Œå¹¶åšäº†å¦‚ä¸‹ä¿®æ”¹ï¼š

**ä¿®æ”¹å‰:**
```html
<link rel="stylesheet" href="{{ "/css/hux-blog.min.css" | prepend: site.baseurl }}">
```

**ä¿®æ”¹å:**
```html
<link rel="stylesheet" href="{{ "/css/hux-blog.min.css" | prepend: site.baseurl }}?v={{ site.time | date: '%s' }}">
```

**è¿™æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ**
*   `site.time` æ˜¯ Jekyll åœ¨**æ„å»ºç½‘ç«™æ—¶**çš„å½“å‰æ—¶é—´ã€‚
*   `date: '%s'` è¿‡æ»¤å™¨å°†è¿™ä¸ªæ—¶é—´æ ¼å¼åŒ–ä¸º **Unix æ—¶é—´æˆ³**ï¼ˆä¸€ä¸ªç‹¬ä¸€æ— äºŒçš„ã€ä¸æ–­å¢é•¿çš„ç§’æ•°ï¼‰ã€‚
*   æœ€ç»ˆç”Ÿæˆçš„ URL ä¼šåƒè¿™æ ·ï¼š`/css/hux-blog.min.css?v=1762041600`ã€‚

æ¯æ¬¡æˆ‘ä»¬çš„ç½‘ç«™é€šè¿‡ GitHub Actions é‡æ–°æ„å»ºæ—¶ï¼Œéƒ½ä¼šç”Ÿæˆä¸€ä¸ªæ–°çš„æ—¶é—´æˆ³ã€‚æµè§ˆå™¨çœ‹åˆ°è¿™ä¸ªæ–°çš„ URLï¼Œå°±ä¼šè®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªå®ƒä»æœªè§è¿‡çš„æ–°æ–‡ä»¶ï¼Œä»è€Œæ”¾å¼ƒæ—§çš„ç¼“å­˜ï¼Œä¸‹è½½æœ€æ–°çš„ç‰ˆæœ¬ã€‚

æˆ‘ä»¬å¯¹æ‰€æœ‰ä¸»è¦çš„ CSS å’Œ JS æ–‡ä»¶éƒ½åº”ç”¨äº†åŒæ ·çš„æ“ä½œã€‚è¿™æ ·ä¸€æ¥ï¼Œå°±å½»åº•è§£å†³äº†å› æµè§ˆå™¨ç¼“å­˜å¯¼è‡´çš„ç”¨æˆ·æ— æ³•çœ‹åˆ°æœ€æ–°æ›´æ–°çš„é—®é¢˜ï¼Œå®Œæˆäº†æˆ‘ä»¬ä»åç«¯æ•°æ®åˆ°å‰ç«¯ä½“éªŒçš„æœ€åä¸€å…¬é‡Œã€‚

## ç¬¬ä¸‰é˜¶æ®µï¼šè‡ªåŠ¨åŒ–ä¸å±•ç¤º

ä»£ç å·²ç»å‡†å¤‡å°±ç»ªï¼Œç°åœ¨æˆ‘ä»¬éœ€è¦è®©å®ƒåœ¨äº‘ç«¯è‡ªåŠ¨è¿è¡Œï¼Œå¹¶å°†ç»“æœå±•ç¤ºåœ¨æˆ‘ä»¬çš„åšå®¢ä¸Šã€‚

### 1. é›†æˆåˆ° GitHub Actions (`.github/workflows/deploy.yml`)

æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®©çˆ¬è™«åªåœ¨**å®šæ—¶ä»»åŠ¡**æˆ–**æ‰‹åŠ¨è§¦å‘**æ—¶è¿è¡Œï¼Œè€Œåœ¨æ™®é€šçš„ `git push` æ—¶è·³è¿‡ï¼Œä»¥èŠ‚çœèµ„æºã€‚æˆ‘ä»¬é€šè¿‡ä¿®æ”¹ `deploy.yml` å·¥ä½œæµæ–‡ä»¶æ¥å®ç°è¿™ä¸€ç‚¹ã€‚

```yaml
# .github/workflows/deploy.yml

# ... (çœç•¥äº† on, permissions, concurrency ç­‰è®¾ç½®) ...

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout ğŸ›ï¸
        uses: actions/checkout@v4

      # æ–°å¢ï¼šè®¾ç½® Python 3.11 ç¯å¢ƒ
      - name: Setup Python ğŸ
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      # æ–°å¢ï¼šå®‰è£… Python ä¾èµ–
      - name: Install Python dependencies ğŸ“¦
        run: pip install -r requirements.txt

      # å…³é”®ï¼æ–°å¢ï¼šå®‰è£… Playwright æµè§ˆå™¨å†…æ ¸
      - name: Install Playwright Browsers ğŸ­
        run: playwright install

      # æ–°å¢ï¼šè¿è¡Œçˆ¬è™«è„šæœ¬ï¼Œå¹¶è®¾ç½®è§¦å‘æ¡ä»¶
      - name: Run crawler ğŸ•·ï¸
        if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
        run: python crawlers/main.py

      - name: Setup Ruby and install gems ğŸ’
        # ... (åŸæœ‰çš„ Jekyll è®¾ç½®æ­¥éª¤)

      - name: Build the site ğŸ—ï¸
        # ... (åŸæœ‰çš„ Jekyll æ„å»ºæ­¥éª¤)

      - name: Upload artifact ğŸ“¦
        # ... (åŸæœ‰çš„ä¸Šä¼ æ­¥éª¤)

# ... (çœç•¥äº† deploy job)
```

**æœ€é‡è¦çš„ä¿®æ”¹æœ‰ä¸‰å¤„ï¼š**
1.  **å®‰è£… Python å’Œä¾èµ–**: ä½¿ç”¨ `actions/setup-python` å¹¶è¿è¡Œ `pip install`ã€‚
2.  **å®‰è£… Playwright æµè§ˆå™¨**: è¿™æ˜¯æˆ‘ä»¬åœ¨è°ƒè¯•ä¸­å‘ç°çš„å…³é”®ä¸€æ­¥ã€‚`crawl4ai` åº•å±‚ä½¿ç”¨ `Playwright`ï¼Œå®ƒéœ€è¦ä¸€ä¸ªçœŸå®çš„æµè§ˆå™¨å†…æ ¸æ‰èƒ½å·¥ä½œã€‚`playwright install` å‘½ä»¤ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶å®‰è£…å®ƒã€‚
3.  **æ¡ä»¶åŒ–è¿è¡Œçˆ¬è™«**: `if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'` è¿™è¡Œä»£ç ç¡®ä¿äº†çˆ¬è™«åªåœ¨æˆ‘ä»¬æƒ³è¦å®ƒè¿è¡Œçš„æ—¶å€™è¿è¡Œã€‚

### 2. å‰ç«¯é¡µé¢å±•ç¤º (`daily.html`)

æœ€åä¸€æ­¥ï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹ `daily.html` é¡µé¢ï¼Œè®©å®ƒèƒ½è¯»å– `_data/` ç›®å½•ä¸‹çš„æ•°æ®å¹¶æ¸²æŸ“å‡ºæ¥ã€‚æˆ‘ä»¬ä½¿ç”¨ Jekyll çš„ `Liquid` æ¨¡æ¿è¯­è¨€æ¥å®ç°ã€‚

```html
--- 
layout: page
title: "Daily"
---

<style>
.post-card { /* ... çœç•¥æ ·å¼ ... */ }
</style>

<div class="container">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            
            {% comment %} 1. æ‰¾åˆ°æœ€æ–°çš„ daily_*.json æ–‡ä»¶ {% endcomment %}
            {% assign latest_date = "1970-01-01" %}
            {% for file in site.data %}
                {% if file[0] contains "daily_" %}
                    {% assign file_date_str = file[0] | remove: "daily_" %}
                    {% if file_date_str > latest_date %}
                        {% assign latest_date = file_date_str %}
                    {% endif %}
                {% endif %}
            {% endfor %}

            {% comment %} 2. è¯»å–å¹¶å±•ç¤ºæœ€æ–°æ–‡ä»¶ä¸­çš„æ–‡ç«  {% endcomment %}
            {% if latest_date != "1970-01-01" %}
                {% assign latest_data_key = "daily_" | append: latest_date %}
                {% assign articles = site.data[latest_data_key] %}

                <p class="post-meta">Showing articles from: {{ latest_date }}</p>

                {% for article in articles %}
                    <a href="{{ article.link }}" target="_blank" class="post-card">
                        <h2>{{ article.title }}</h2>
                        <p class="post-meta">Source: cnblogs</p>
                    </a>
                {% endfor %}
            {% else %}
                <p>No daily information available yet. Please run the crawler.</p>
            {% endif %}

        </div>
    </div>
</div>
```
è¿™æ®µä»£ç çš„é€»è¾‘å¾ˆæ¸…æ™°ï¼šé¦–å…ˆéå† `site.data` æ‰¾åˆ°æœ€æ–°çš„æ—¥æœŸï¼Œç„¶ååŠ è½½å¯¹åº”çš„æ•°æ®æ–‡ä»¶ï¼Œæœ€åé€šè¿‡ä¸€ä¸ª `for` å¾ªç¯å°†æ¯ç¯‡æ–‡ç« æ¸²æŸ“æˆä¸€ä¸ªå¯ç‚¹å‡»çš„å¡ç‰‡ã€‚
