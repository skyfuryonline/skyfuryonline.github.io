---
layout: post
title: "基于 Python 的自动化 Jekyll 内容聚合系统"
date: 2025-11-06
author: "Gemini"
tags: [Jekyll, Python, Selenium, GitHub Actions, 自动化]
group: blog-development
---

## 引言

要保持一个技术博客的新鲜度和吸引力，持续分享高质量、高相关性的内容至关重要。然而，手动地在网络上搜索、筛选和发布文章是一个极其耗时的过程。为了解决这个问题，我为本博客开发了一套完全自动化的内容聚合系统。该系统能够自动发现、抓取、总结并展示我所喜爱的技术博客的最新内容，整个流程由 GitHub Actions 编排。

本文将详细介绍这套强大的自动化策略的系统架构与数据流。

## 核心架构

该系统由以下几个关键组件协同工作：

-   **GitHub Actions**: 作为核心的编排与调度器，每天自动运行整个流程。
-   **Python**: 后端逻辑的核心语言，处理从网页抓取到数据处理的所有事务。
-   **Selenium**: 一个强大的网页自动化工具，用于控制一个无头 (headless) 的 Chrome 浏览器。这使我们能够渲染并抓取那些原本无法被简单 HTTP 库访问的、重度依赖 JavaScript 的现代网站。
-   **aiohttp**: 一个异步 HTTP 客户端，用于高效地并行下载一篇文章中的所有图片文件。

## 爬虫模块 (`crawlers/`)

系统的核心位于 `crawlers/` 目录中，它遵循一个清晰的、分两层的设计模式：

### 1. 编排器 (`main.py`)

这个脚本是整个操作的大脑。其职责包括：

-   **驱动管理**: 初始化一个唯一的、共享的 Selenium WebDriver 实例，并将其传递给所有独立的爬虫。这远比每个爬虫都启动自己的浏览器实例要高效得多。
-   **配置读取**: 从 `config.json` 读取配置，以确定要抓取哪些网站、抓取多少篇文章，以及使用哪种 LLM 提示词进行总结。
-   **数据处理**: 接收来自爬虫的原始数据（文本和图片 URL）。
-   **文件缓存**: 处理所有文件系统操作，为每篇文章创建一个干净的缓存结构（例如 `cache/YYYY-MM-DD/article-title/`），并保存内容文本与图片。
-   **LLM 集成**: 将文章内容发送给大语言模型，以获得一段精炼的摘要。
-   **数据生成**: 将最终的结构化元数据（标题、链接、来源、摘要、相对缓存路径、图片文件名等）编译成一个 `daily_YYYY-MM-DD.json` 文件，供 Jekyll 使用。

### 2. 具体爬虫 (`specific_crawlers/`)

此目录中的每个文件都是一个继承自 `BaseCrawler` 的类，各自负责一个目标网站。这里的设计哲学是**关注点分离**：爬虫的唯一工作是**抓取数据**，而不是处理或保存它。

对于一个给定的网站，具体的爬虫会：
1.  访问主博客页面。
2.  找到最新文章的链接。
3.  对于每一篇新文章，访问其详情页，提取主要内容的文本，并找出内容中所有图片的 URL。
4.  返回一个包含字典的列表，每个字典都含有文章的标题、链接、全文文本，以及一个包含其所有图片 URL 的列表。

## 数据流转：从网络到博客

端到端的数据流转过程如下：

1.  **触发**: GitHub Actions 按预定时间启动 `main.py` 脚本。
2.  **抓取**: 爬虫访问如“谷歌开发者博客”等网站，使用 Selenium 渲染页面，并收集文章文本和图片 URL。
3.  **处理**: `main.py` 接收数据。对于一篇标题为“我的精彩文章”的文章，它会创建一个目录 `cache/2025-11-06/my-awesome-post/`。
4.  **缓存**: 脚本将文章文本保存到 `content.txt`，并异步下载所有相关的图片（例如 `image_1.png`, `image_2.webp`）到该目录中。
5.  **总结**: 文章内容被发送到 LLM，返回的摘要被添加到文章的元数据中。
6.  **生成**: `_data/daily_2025-11-06.json` 文件被创建，其中包含“我的精彩文章”的条目，内容大致如下：

    ```json
    {
      "title": "我的精彩文章",
      "link": "https://example.com/my-awesome-post",
      "source": "GoogleDevBlogCrawler",
      "summary": "这篇文章是关于...",
      "cache_path": "cache/2025-11-06/my-awesome-post",
      "image_files": ["image_1.png", "image_2.webp"]
    }
    ```
7.  **渲染**: Jekyll 站点被重建。`daily.html` 页面会读取这个 JSON 文件，并使用 Liquid 模板来展示标题和来源。当用户点击卡片时，一个模态窗口会弹出，显示摘要，并使用 `cache_path` 和 `image_files` 字段来构建正确的、相对的图片 URL。

## 结论

这套自动化系统提供了一种强大的、零接触的方式，来保持博客内容的更新、相关和美观。通过利用现代化的 Python 技术栈和 GitHub Actions 的强大能力，它将一项繁琐的手工任务，转变成了一个无缝的、每日运行的后台作业。
