---
layout: post
title: "åšå®¢çˆ¬è™«é›†æˆ(ç»­)ï¼šèµ‹äºˆAIæ‘˜è¦èƒ½åŠ›"
subtitle: "é€šè¿‡LLM APIä¸å‰ç«¯æ¨¡æ€æ¡†ï¼Œå®ç°æ–‡ç« çš„è‡ªåŠ¨æ€»ç»“ä¸ä¼˜é›…å±•ç¤º"
date: 2025-11-04 10:00:00
author: "LH"
tags: [LLM, API, JavaScript, Jekyll, CI/CD]
group: life
---

## å‰è¨€ï¼šä»â€œæ˜¯ä»€ä¹ˆâ€åˆ°â€œè®²ä»€ä¹ˆâ€

åœ¨ä¸Šä¸€ç¯‡æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬æˆåŠŸåœ°ä¸ºåšå®¢é›†æˆäº†ä¸€ä¸ªå…¨è‡ªåŠ¨çš„çˆ¬è™«ï¼Œå®ƒèƒ½æ¯å¤©æŠ“å–æœ€æ–°çš„æ–‡ç« åˆ—è¡¨ã€‚ä½†è¿™åªè§£å†³äº†â€œæœ‰ä»€ä¹ˆæ–°å†…å®¹â€çš„é—®é¢˜ã€‚ç”¨æˆ·çœ‹åˆ°æ ‡é¢˜åï¼Œä¾ç„¶éœ€è¦è·³è½¬åˆ°åŸæ–‡æ‰èƒ½åˆ¤æ–­æ˜¯å¦å€¼å¾—ä¸€è¯»ï¼Œè¿™åœ¨ä¿¡æ¯è¿‡è½½çš„æ—¶ä»£æ•ˆç‡ä¸é«˜ã€‚

æˆ‘ä»¬èƒ½å¦æ›´è¿›ä¸€æ­¥ï¼Œè®© AI å‘Šè¯‰æˆ‘ä»¬æ¯ç¯‡æ–‡ç« **â€œè®²äº†ä»€ä¹ˆâ€**ï¼Ÿ

è¿™ç¯‡æ•™ç¨‹ï¼Œæˆ‘ä»¬å°†åœ¨æ­¤å‰çš„çˆ¬è™«åŸºç¡€ä¸Šï¼Œé›†æˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºå¤§èƒ½åŠ›ï¼Œå®ç°å¯¹æŠ“å–æ–‡ç« çš„**è‡ªåŠ¨æ‘˜è¦**åŠŸèƒ½ã€‚æœ€ç»ˆæ•ˆæœæ˜¯ï¼Œç”¨æˆ·åœ¨ "Daily" é¡µé¢ç‚¹å‡»ä¸€ç¯‡æ–‡ç« ï¼Œä¼šå…ˆå¼¹å‡ºä¸€ä¸ªç”± AI ç”Ÿæˆçš„æ‘˜è¦ï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿå†³ç­–ï¼Œæå¤§åœ°æå‡ä¿¡æ¯è·å–æ•ˆç‡ã€‚

## è®¾è®¡å“²å­¦ï¼šâ€œé¢„ç”Ÿæˆâ€ä¸â€œçº¯é™æ€â€

åœ¨çº¯é™æ€çš„ GitHub Pages ç¯å¢ƒä¸‹ï¼Œæˆ‘ä»¬æ— æ³•åœ¨ç”¨æˆ·ç‚¹å‡»æ—¶ï¼Œå»å®æ—¶è°ƒç”¨åç«¯æœåŠ¡å’Œ LLM APIã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»è½¬å˜æ€è·¯ï¼Œé‡‡ç”¨**â€œé¢„ç”Ÿæˆ (Pre-generation)â€**çš„ç­–ç•¥ã€‚

**æ ¸å¿ƒæ€æƒ³ï¼š** å°†æ‰€æœ‰è®¡ç®—å¯†é›†å‹ã€éœ€è¦åç«¯å‚ä¸çš„å·¥ä½œï¼ˆè°ƒç”¨ LLM APIï¼‰ï¼Œå…¨éƒ¨å‰ç½®åˆ°**æ„å»ºé˜¶æ®µ (Build Time)** å®Œæˆã€‚

**å…·ä½“æµç¨‹ï¼š**
1.  çˆ¬è™«åœ¨ Actions ç¯å¢ƒä¸­æŠ“å–åˆ°åŸæ–‡å†…å®¹ã€‚
2.  **ç«‹å³**å°†åŸæ–‡å†…å®¹å‘é€ç»™ LLM APIï¼Œè·å–æ‘˜è¦ã€‚
3.  å°†**æ‘˜è¦**å’Œæ–‡ç« æ ‡é¢˜ã€é“¾æ¥ç­‰å…ƒæ•°æ®**ä¸€åŒ**å†™å…¥æœ€ç»ˆçš„ `_data/daily_...json` æ–‡ä»¶ã€‚
4.  å‰ç«¯é¡µé¢åŠ è½½æ—¶ï¼Œæ‘˜è¦ä¿¡æ¯å·²ç»å­˜åœ¨äºæ•°æ®ä¸­ï¼Œåªéœ€é€šè¿‡ JavaScript å°†å…¶å±•ç¤ºå‡ºæ¥å³å¯ã€‚

è¿™ä¸ªæ–¹æ¡ˆå®Œç¾åœ°å¥‘åˆäº†é™æ€ç½‘ç«™çš„å“²å­¦ï¼Œä¿è¯äº†ç”¨æˆ·è®¿é—®æ—¶æè‡´çš„é€Ÿåº¦å’Œä½“éªŒï¼ŒåŒæ—¶ç¡®ä¿äº† API å¯†é’¥çš„ç»å¯¹å®‰å…¨ã€‚

## ç¬¬ä¸€æ­¥ï¼šAPI å¯†é’¥çš„å®‰å…¨ç®¡ç†

åœ¨ä¸ä»»ä½•éœ€è¦ä»˜è´¹çš„ API äº¤äº’æ—¶ï¼Œå®‰å…¨æ°¸è¿œæ˜¯ç¬¬ä¸€ä½çš„ã€‚æˆ‘ä»¬ç»ä¸èƒ½å°† API å¯†é’¥ç¡¬ç¼–ç åœ¨ä»£ç ä¸­ã€‚

**å”¯ä¸€æ­£ç¡®çš„æ–¹å¼ï¼š** ä½¿ç”¨ GitHub Secretsã€‚

1.  å‰å¾€æ‚¨çš„ GitHub ä»“åº“é¡µé¢ï¼Œç‚¹å‡» `Settings` > `Secrets and variables` > `Actions`ã€‚
2.  ç‚¹å‡» `New repository secret`ã€‚
3.  åˆ›å»ºä¸€ä¸ªåä¸º `LLM_API_KEY` çš„ Secretï¼Œå¹¶å°†æ‚¨çš„ API å¯†é’¥ç²˜è´´è¿›å»ã€‚
4.  (å¯é€‰) å¦‚æœæ‚¨ä½¿ç”¨ä»£ç†æˆ–è‡ªæ‰˜ç®¡æœåŠ¡ï¼Œå¯ä»¥å†åˆ›å»ºä¸€ä¸ª `LLM_API_BASE_URL` çš„ Secretã€‚

è¿™æ ·ï¼Œå¯†é’¥å°±è¢«åŠ å¯†å­˜å‚¨äº†ï¼Œåªæœ‰åœ¨ Actions è¿è¡Œæ—¶æ‰èƒ½è¢«æˆ‘ä»¬çš„è„šæœ¬è¯»å–ã€‚

## ç¬¬äºŒæ­¥ï¼šå¯æ‰©å±•çš„é…ç½® (`config.json`)

ä¸ºäº†æ–¹ä¾¿æœªæ¥ä¸ºä¸åŒç½‘ç«™ï¼ˆå¦‚æŠ€æœ¯åšå®¢ã€è®ºæ–‡ç½‘ç«™ï¼‰åº”ç”¨ä¸åŒçš„æ€»ç»“æ¨¡å‹æˆ–æŒ‡ä»¤ (Prompt)ï¼Œæˆ‘ä»¬å°† `config.json` å‡çº§ä¸ºå¯é…ç½®çš„â€œæ€»ç»“ç­–ç•¥â€ã€‚

```json
{
  "sites": [
    {
      "url": "https://www.cnblogs.com/pick/",
      "parser": "CnblogsCrawler",
      "llm_profile": "default_summary" 
    }
  ],
  "llm_profiles": {
    "default_summary": {
      "model": "gpt-3.5-turbo",
      "prompt": "ä½ æ˜¯ä¸€ä¸ªä¼˜ç§€çš„å†…å®¹æ‘˜è¦åŠ©æ‰‹ã€‚è¯·å°†ä»¥ä¸‹æ–‡ç« å†…å®¹æ€»ç»“ä¸ºä¸€æ®µ150å­—ä»¥å†…çš„ä¸­æ–‡æ‘˜è¦ï¼Œæå–æ ¸å¿ƒè§‚ç‚¹å’Œä¸»è¦ä¿¡æ¯ï¼Œåªè¾“å‡ºæ‘˜è¦æœ¬èº«ï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è¯ã€‚"
    },
    "academic_summary": {
      "model": "gpt-4",
      "prompt": "è¯·å°†ä»¥ä¸‹è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ã€ä½¿ç”¨çš„æ–¹æ³•å’Œæœ€ç»ˆçš„å®éªŒç»“æœï¼Œæ€»ç»“ä¸ºä¸‰ç‚¹ï¼Œæ¯ç‚¹ä¸è¶…è¿‡50å­—ã€‚"
    }
  }
}
```
é€šè¿‡ `llm_profiles`ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºä¸åŒç±»å‹çš„æ–‡ç« å®šä¹‰ä¸åŒçš„æ€»ç»“â€œé…æ–¹â€ï¼Œæœªæ¥åªéœ€ä¿®æ”¹è¿™ä¸ª JSON æ–‡ä»¶ï¼Œå°±èƒ½è½»æ¾è°ƒæ•´æ€»ç»“ç­–ç•¥ï¼Œæ— éœ€æ”¹åŠ¨ä»»ä½• Python ä»£ç ã€‚

## ç¬¬ä¸‰æ­¥ï¼šç‹¬ç«‹çš„ LLM è°ƒç”¨æ¨¡å— (`llm/summarizer.py`)

æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„æ¨¡å—ï¼Œä¸“é—¨è´Ÿè´£ä¸ LLM API é€šä¿¡ã€‚è¿™é‡Œæˆ‘ä»¬ä»¥ä»»ä½•æ”¯æŒ OpenAI å…¼å®¹ API çš„æœåŠ¡ä¸ºä¾‹ã€‚

```python
# llm/summarizer.py

import os
from openai import OpenAI

client = None

def initialize_client():
    """åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼Œä»¥ä¾¿å¤ç”¨ã€‚"""
    global client
    api_key = os.environ.get('LLM_API_KEY')
    base_url = os.environ.get('LLM_API_BASE_URL')
    if api_key and client is None:
        client = OpenAI(api_key=api_key, base_url=base_url)

def get_summary(content: str, model: str, prompt_template: str) -> str:
    """è°ƒç”¨ LLM API è·å–æ‘˜è¦ã€‚"""
    initialize_client()
    
    if not client:
        return "Error: LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ LLM_API_KEYã€‚"
    if not content:
        return "(æ–‡ç« å†…å®¹ä¸ºç©ºï¼Œæœªç”Ÿæˆæ‘˜è¦)"

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": prompt_template},
                {"role": "user", "content": content[:15000]} # æˆªæ–­å†…å®¹ä»¥é˜²è¶…é•¿
            ],
            temperature=0.5,
            timeout=180
        )
        summary = response.choices[0].message.content.strip()
        return summary
    except Exception as e:
        return f"è°ƒç”¨ LLM API æ—¶å‡ºé”™: {e}"
```

## ç¬¬å››æ­¥ï¼šä¸²è”æµç¨‹ (`crawlers/main.py`)

ç°åœ¨ï¼Œæˆ‘ä»¬æ”¹é€ â€œå¤§è„‘â€â€”â€”`main.py`ï¼Œè®©å®ƒåœ¨çˆ¬å–åˆ°æ–‡ç« åï¼Œè°ƒç”¨ `summarizer` æ¥ç”Ÿæˆæ‘˜è¦ã€‚

```python
# crawlers/main.py (æ ¸å¿ƒä¿®æ”¹éƒ¨åˆ†)

# ... (åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥)
from llm.summarizer import get_summary

async def main():
    # ... (çœç•¥äº†è·¯å¾„è®¾ç½®ã€åŠ è½½å†å²ç­‰ä»£ç )

    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    llm_profiles = config.get("llm_profiles", {})

    all_articles_metadata = []
    for site in config["sites"]:
        # ... (çœç•¥äº†çˆ¬è™«å®ä¾‹åŒ–ä»£ç )
        articles_metadata = await crawler_instance.crawl()

        # --- LLM é›†æˆå¼€å§‹ --- #
        llm_profile_name = site.get("llm_profile")
        if llm_profile_name and llm_profile_name in llm_profiles:
            profile = llm_profiles[llm_profile_name]
            print(f"ä½¿ç”¨ LLM é…ç½® '{llm_profile_name}' ç”Ÿæˆæ‘˜è¦...")
            for article in articles_metadata:
                try:
                    # 1. è¯»å–ç¼“å­˜çš„åŸæ–‡
                    with open(os.path.join(article['cache_path'], 'content.txt'), 'r', encoding='utf-8') as content_file:
                        content = content_file.read()
                    
                    # 2. è°ƒç”¨ summarizer è·å–æ‘˜è¦
                    summary = get_summary(content, profile['model'], profile['prompt'])
                    article['summary'] = summary # 3. å°†æ‘˜è¦æ·»åŠ åˆ°å…ƒæ•°æ®ä¸­
                    print(f"  - å·²æ€»ç»“: {article['title']}")
                except Exception as e:
                    article['summary'] = f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}"
        
        all_articles_metadata.extend(articles_metadata)

    # ... (ä¿å­˜æœ€ç»ˆæ•°æ®å’Œæ¸…ç†æ—§æ•°æ®çš„ä»£ç )
```

## ç¬¬äº”æ­¥ï¼šæ³¨å…¥ API å¯†é’¥ (`deploy.yml`)

æœ€åï¼Œæˆ‘ä»¬ä¿®æ”¹ `.github/workflows/deploy.yml`ï¼Œé€šè¿‡ `env` å…³é”®å­—ï¼Œå°†æˆ‘ä»¬è®¾ç½®çš„ Secret å®‰å…¨åœ°æ³¨å…¥åˆ°çˆ¬è™«è„šæœ¬çš„è¿è¡Œç¯å¢ƒä¸­ã€‚

```yaml
      - name: Run crawler ğŸ•·ï¸
        if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
        env:
          LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
          LLM_API_BASE_URL: ${{ secrets.LLM_API_BASE_URL }} # å¯é€‰
        run: python crawlers/main.py
```

è‡³æ­¤ï¼Œæˆ‘ä»¬çš„åç«¯å’Œè‡ªåŠ¨åŒ–æµæ°´çº¿å·²ç»å‡†å¤‡å°±ç»ªã€‚ä¸‹ä¸€æ¬¡å½“å®šæ—¶ä»»åŠ¡è¿è¡Œæ—¶ï¼Œå®ƒç”Ÿæˆçš„ `_data/daily_...json` æ–‡ä»¶ä¸­ï¼Œå°†åŒ…å«ä¸€ä¸ªå…¨æ–°çš„ `summary` å­—æ®µã€‚æˆ‘ä»¬çš„ä¸‹ä¸€æ­¥ï¼Œå°±æ˜¯åœ¨å‰ç«¯é¡µé¢ä¸Šï¼Œå°†è¿™ä¸ªæ‘˜è¦ä¼˜é›…åœ°å±•ç¤ºç»™ç”¨æˆ·ã€‚
