Breadcrumb
Innovation & AI
Technology
Developer tools
Introducing Community Benchmarks on Kaggle
Jan 14, 2026
·
2 min read
Read AI-generated summary
General summary
Kaggle launched Community Benchmarks so you can design and share custom benchmarks for evaluating AI models. You can build tasks to test model performance on specific problems. Group those tasks into a benchmark to evaluate leading AI models and track their performance on a leaderboard.
Summaries were generated by Google AI. Generative AI is experimental.
Bullet points
"Introducing Community Benchmarks on Kaggle" lets the AI community design and share custom AI model evaluations.
Community Benchmarks offer a transparent way to validate specific use cases for AI model performance.
Build tasks to test AI models, then group them into benchmarks to compare model performance.
You'll get free access to models, reproducible results, complex interaction testing, and rapid prototyping.
Kaggle's Community Benchmarks help shape the future of AI by improving how models are evaluated.
Summaries were generated by Google AI. Generative AI is experimental.
Explore other styles:
General summary
Bullet points
Share
x.com
Facebook
LinkedIn
Mail
Copy link
Today’s AI models require more than static accuracy scores. Community Benchmarks, a new capability on Kaggle, enables the global AI community to design, run and share custom evaluations that better reflect real-world model behavior.
Michael Aaron
Software Engineer, Kaggle
Meg Risdal
Product Lead, Kaggle
Share
x.com
Facebook
LinkedIn
Mail
Copy link
Your browser does not support the audio element.
Listen to article
This content is generated by Google AI. Generative AI is experimental
4:02 minutes
0:00
4:02
Voice
Umbriel
Speed
1X
Voice
Umbriel
Gacrux
Speed
0.75X
1X
1.5X
2X
Today, Kaggle is launching
Community Benchmarks
, which lets the global AI community design, run and share their own custom benchmarks for evaluating AI models. This is the next step after we launched
Kaggle Benchmarks last year,
to provide trustworthy and transparent access to evaluations from top-tier research groups like
Meta’s MultiLoKo
and
Google’s FACTS suite
.
Why community-driven evaluation matters
AI capabilities have evolved so rapidly that it’s become difficult to evaluate model performance. Not long ago, a single accuracy score on a static dataset was enough to determine model quality. But today, as LLMs evolve into reasoning agents that collaborate, write code and use tools, those static metrics and simple evaluations are no longer sufficient.
Kaggle Community Benchmarks provide developers with a transparent way to validate their specific use cases and bridge the gap between experimental code and production-ready applications.
These real-world use cases demand a more flexible and transparent evaluation framework. Kaggle’s Community Benchmarks provide a more dynamic, rigorous and continuously evolving approach to AI model evaluation — one shaped by the users building and deploying these systems everyday.
How to build your own benchmarks on Kaggle
Benchmarks start with building tasks, which can range from evaluating multi-step reasoning and code generation to testing tool use or image recognition. Once you have tasks, you can add them to a benchmark to evaluate and rank selected models by how they perform across the tasks in the benchmark.
Here’s how you can get started:
Create a task:
Tasks test an AI model’s performance on a specific problem. They allow you to run reproducible tests across different models to compare their accuracy and capabilities.
Create a benchmark:
Once you have created one or more tasks, you can group them into a Benchmark. A benchmark allows you to run tasks across a suite of leading AI models and generate a leaderboard to track and compare their performance.
00:00
Once you build your benchmark, here’s what benefits you’ll see:
Broad model access:
Free access (within quota limits) to state-of-the-art models from labs like Google, Anthropic, DeepSeek and more.
Reproducibility:
Benchmarks capture exact outputs and model interactions so results can be audited and verified.
Complex interactions:
They support testing for multi-modal inputs, code execution, tool use and multi-turn conversations.
Rapid prototyping:
They allow you to quickly design and iterate on creative new tasks.
These powerful capabilities are powered by the new
kaggle-benchmarks SDK
. Here are a few resources for getting started:
Benchmarks Cookbook:
A guide to advanced features and use cases.
Example tasks:
Get inspired with a variety of pre-built tasks.
Getting started
:
How to create your first task & benchmark
How we’re shaping the future of AI evaluation
The future of AI progress depends on how models are evaluated. With Kaggle Community Benchmarks, Kagglers are no longer just testing models, they’re helping shape the next generation of intelligence.
Ready to build? Try
Community Benchmarks
today.
POSTED IN:
Developer tools
AI
Related stories
Google in the Middle East
Announcing the winner of the Global AI Film Award
By
Anthony Nakache
Jan 14, 2026
AI
Veo 3.1 Ingredients to Video: More consistency, creativity and control
By
Ricky Wong
Jan 13, 2026
Developer tools
Enhanced Veo 3.1 capabilities are now available in the Gemini API.
By
Alisa Fortin
Jan 13, 2026
Developer tools
Increased file size limits and expanded inputs support in Gemini API
By
Lucia Loher
Jan 12, 2026
AI Products
The latest AI news we announced in December
By
Keyword Team
Dec 29, 2025
Developer tools
T5Gemma 2: The next generation of encoder-decoder models
By
Biao Zhang
&
Ben Hora
Dec 18, 2025
.
Jump to position 1
Jump to position 2
Jump to position 3
Jump to position 4
Jump to position 5
Jump to position 6