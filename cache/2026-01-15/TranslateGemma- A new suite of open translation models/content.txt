Breadcrumb
Innovation & AI
Technology
Developer tools
TranslateGemma: A new suite of open translation models
Jan 15, 2026
·
3 min read
Share
x.com
Facebook
LinkedIn
Mail
Copy link
Today, we're introducing TranslateGemma, a new collection of open translation models built on Gemma 3, helping people communicate across 55 languages, no matter where they are or what device they own.
D
David Vilar
Staff Research Scientist
K
Kat Black
Product Manager
Share
x.com
Facebook
LinkedIn
Mail
Copy link
Today, we're introducing TranslateGemma, a new collection of open translation models built on Gemma 3, available in 4B, 12B, and 27B parameter sizes. It represents a significant step forward in open translation, helping people communicate across 55 languages, no matter where they are or what device they own.
By distilling the knowledge of our most advanced large models into compact, high-performance open models, we have created a suite where efficiency doesn't require a compromise on quality.
Outperforming models twice its size
The most remarkable finding in our technical evaluation is the efficiency of these models. Through our specialized training process, the 12B TranslateGemma model outperforms the Gemma 3 27B baseline as measured using MetricX on the WMT24++ benchmark.
For developers, this is a massive win. You can achieve high-fidelity translation quality using less than half the parameters of the baseline model. This efficiency breakthrough allows for higher throughput and lower latency without sacrificing accuracy. Similarly, the 4B model rivals the performance of the larger 12B baseline, making it a powerful model for mobile inference.
We tested TranslateGemma on the WMT24++ dataset, comprising 55 languages covering a wide variety of language families, including high-, mid- and low-resource languages. TranslateGemma considerably reduced the error rate compared to the baseline Gemma model in all languages, achieving improved quality with greater efficiency.
Built from Gemini
How did we achieve this density of intelligence? It comes down to a specialized two-stage fine-tuning process that distills the "intuition" of our Gemini models into an open architecture.
Supervised Fine-Tuning (SFT):
We fine-tuned the base Gemma 3 models on a diverse dataset of parallel data. This dataset includes a rich mix of human-translated texts and high-quality synthetic translations generated by state-of-the-art Gemini models, achieving broad language coverage and high fidelity even in low-resource languages.
Reinforcement Learning (RL):
To further refine the translation quality, we implemented a novel reinforcement learning phase. We used an ensemble of reward models, including advanced metrics like MetricX-QE and AutoMQM, to guide the models towards producing more contextually accurate and natural-sounding translations.
Unprecedented language coverage
We have rigorously trained and evaluated TranslateGemma on 55 language pairs, ensuring reliable, high-quality performance across major languages (such as Spanish, French, Chinese, and Hindi) as well as many low-resource languages.
Beyond these core languages, we pushed the boundaries by training on nearly 500 additional language pairs. We designed TranslateGemma to serve as a robust foundation for further adaptation, making it an ideal starting point for researchers to fine-tune their own state-of-the-art models for specific language pairs or to improve quality for low-resource languages. While we do not yet have confirmed evaluation metrics for this extended set, we have included the full list in our
technical report
to encourage community exploration and further research.
Strong multimodal capabilities
TranslateGemma models retain the strong multimodal capabilities of Gemma 3. Our tests on the Vistra image translation benchmark show that the improvements in text translation also positively impact the ability to translate text within images, even without specific multimodal fine-tuning during the TranslateGemma training process.
Runs everywhere
TranslateGemma sets a new standard for open translation models, balancing state-of-the-art performance with exceptional efficiency. Available in three sizes, these models are designed for diverse deployment environments:
4B Model:
Optimized for mobile and edge deployment.
12B Model:
Designed to run smoothly on consumer laptops, bringing research-grade power to local development environments.
27B Model:
Built for maximum fidelity, capable of running on a single H100 GPU or TPU in the cloud.
How to try TranslateGemma today
The release of TranslateGemma provides researchers and developers with powerful and adaptable tools for a wide array of translation-related tasks. We are excited to see how the community will build upon and utilize these models to break down language barriers and foster greater understanding across cultures. Here’s how to try it:
Read the technical report
Download on Kaggle
Download on Hugging Face
Explore via the Gemma Cookbook
Deploy in Vertex AI
POSTED IN:
Developer tools
Related stories
Developer tools
Introducing Community Benchmarks on Kaggle
By
Michael Aaron
&
Meg Risdal
Jan 14, 2026
Developer tools
Enhanced Veo 3.1 capabilities are now available in the Gemini API.
By
Alisa Fortin
Jan 13, 2026
Developer tools
Increased file size limits and expanded inputs support in Gemini API
By
Lucia Loher
Jan 12, 2026
AI Products
The latest AI news we announced in December
By
Keyword Team
Dec 29, 2025
Developer tools
T5Gemma 2: The next generation of encoder-decoder models
By
Biao Zhang
&
Ben Hora
Dec 18, 2025
Developer tools
FunctionGemma: Bringing bespoke function calling to the edge
By
Kat Black
&
Ravin Kumar
Dec 18, 2025
.
Jump to position 1
Jump to position 2
Jump to position 3
Jump to position 4
Jump to position 5
Jump to position 6